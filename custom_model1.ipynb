{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid single feature nonlinear regression\n",
    "@LaurentMombaerts 14/07/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom1.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from arch import arch_model\n",
    "\n",
    "def log_return(list_stock_prices): # Stock prices are estimated through wap values\n",
    "    return np.log(list_stock_prices).diff() \n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "def compute_wap(book_pd):\n",
    "    wap = (book_pd['bid_price1'] * book_pd['ask_size1'] + book_pd['ask_price1'] * book_pd['bid_size1']) / (book_pd['bid_size1']+ book_pd['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def realized_volatility_per_time_id(file_path, prediction_column_name):\n",
    "    df_book_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Estimate stock price per time point\n",
    "    df_book_data['wap'] = compute_wap(df_book_data)\n",
    "    \n",
    "    # Compute log return from wap values per time_id\n",
    "    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
    "    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n",
    "    \n",
    "    # Compute the square root of the sum of log return squared to get realized volatility\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n",
    "    \n",
    "    # Formatting\n",
    "    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    \n",
    "    return df_realized_vol_per_stock[['row_id',prediction_column_name]]\n",
    "\n",
    "def past_realized_volatility_per_stock(list_file,prediction_column_name):\n",
    "    df_past_realized = pd.DataFrame()\n",
    "    for file in list_file:\n",
    "        df_past_realized = pd.concat([df_past_realized,\n",
    "                                     realized_volatility_per_time_id(file,prediction_column_name)])\n",
    "    return df_past_realized\n",
    "\n",
    "def stupidForestPrediction(book_path_train,prediction_column_name,train_targets_pd,book_path_test):\n",
    "    naive_predictions_train = past_realized_volatility_per_stock(list_file=book_path_train,prediction_column_name=prediction_column_name)\n",
    "    df_joined_train = train_targets_pd.merge(naive_predictions_train[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "    \n",
    "    X = np.array(df_joined_train['pred']).reshape(-1,1)\n",
    "    y = np.array(df_joined_train['target']).reshape(-1,)\n",
    "\n",
    "    regr = RandomForestRegressor(random_state=0)\n",
    "    regr.fit(X, y)\n",
    "    \n",
    "    naive_predictions_test = past_realized_volatility_per_stock(list_file=book_path_test,prediction_column_name='target')\n",
    "    yhat = regr.predict(np.array(naive_predictions_test['target']).reshape(-1,1))\n",
    "    \n",
    "    updated_predictions = naive_predictions_test.copy()\n",
    "    updated_predictions['target'] = yhat\n",
    "    \n",
    "    return updated_predictions\n",
    "\n",
    "\n",
    "# GARCH PREDICTIONS\n",
    "\n",
    "def garch_fit_predict_volatility(returns_series, N=10000):\n",
    "    model = arch_model(returns_series * N, p=1, q=1)\n",
    "    model_fit = model.fit(update_freq=0, disp='off')\n",
    "    yhat = model_fit.forecast(horizon=600, reindex=False)\n",
    "    \n",
    "    pred_volatility = np.sqrt(np.sum(yhat.variance.values)) / N\n",
    "    \n",
    "    return pred_volatility\n",
    "\n",
    "def garch_volatility_per_time_id(file_path, prediction_column_name):\n",
    "    # read the data\n",
    "    df_book_data = pd.read_parquet(file_path) \n",
    "    \n",
    "    # calculate the midprice (not the WAP)  \n",
    "    df_book_data['midprice'] =(df_book_data['bid_price1'] + df_book_data['ask_price1'])/2\n",
    "    \n",
    "    # leave only the midprice for now\n",
    "    df_book_data = df_book_data[['time_id', 'seconds_in_bucket', 'midprice']]\n",
    "    df_book_data = df_book_data.sort_values('seconds_in_bucket')\n",
    "    \n",
    "    # make the book updates evenly spaced\n",
    "    df_book_data_evenly = pd.DataFrame({'time_id':np.repeat(df_book_data['time_id'].unique(), 600), \n",
    "                                        'second':np.tile(range(0,600), df_book_data['time_id'].nunique())})\n",
    "    df_book_data_evenly['second'] = df_book_data_evenly['second'].astype(np.int16)\n",
    "    df_book_data_evenly = df_book_data_evenly.sort_values('second')\n",
    "    \n",
    "    \n",
    "    df_book_data_evenly = pd.merge_asof(df_book_data_evenly,\n",
    "                           df_book_data,\n",
    "                           left_on='second',right_on='seconds_in_bucket',\n",
    "                           by = 'time_id')\n",
    "\n",
    "    # Ordering for easier use\n",
    "    df_book_data_evenly = df_book_data_evenly[['time_id', 'second', 'midprice']]\n",
    "    df_book_data_evenly = df_book_data_evenly.sort_values(['time_id','second']).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # calculate log returns \n",
    "    df_book_data_evenly['log_return'] = df_book_data_evenly.groupby(['time_id'])['midprice'].apply(log_return)\n",
    "    df_book_data_evenly = df_book_data_evenly[~df_book_data_evenly['log_return'].isnull()]\n",
    "\n",
    "    \n",
    "    # fit GARCH(1, 1) and predict the volatility of returns\n",
    "    df_garch_vol_per_stock =  \\\n",
    "        pd.DataFrame(df_book_data_evenly.groupby(['time_id'])['log_return'].agg(garch_fit_predict_volatility)).reset_index()\n",
    "    df_garch_vol_per_stock = df_garch_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n",
    "    \n",
    "    # add row_id column to the data\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_garch_vol_per_stock['row_id'] = df_garch_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    \n",
    "    # return the result\n",
    "    return df_garch_vol_per_stock[['row_id', prediction_column_name]]\n",
    "\n",
    "def garch_volatility_per_stock(list_file, prediction_column_name):\n",
    "    df_garch_predicted = pd.DataFrame()\n",
    "    for file in list_file:\n",
    "        df_garch_predicted = pd.concat([df_garch_predicted,\n",
    "                                     garch_volatility_per_time_id(file, prediction_column_name)])\n",
    "    return df_garch_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes here**\n",
    "\n",
    "- Training set: \n",
    "- Test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
