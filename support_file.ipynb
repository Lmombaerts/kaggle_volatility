{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support file (functions)\n",
    "@LaurentMombaerts 15/07/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting support_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile support_file.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import time \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from information_measures import *\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def log_return(list_stock_prices): # Stock prices are estimated through wap values\n",
    "    return np.log(list_stock_prices).diff() \n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "def compute_wap(book_pd):\n",
    "    wap = (book_pd['bid_price1'] * book_pd['ask_size1'] + book_pd['ask_price1'] * book_pd['bid_size1']) / (book_pd['bid_size1']+ book_pd['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def realized_volatility_from_book_pd(book_stock_time):\n",
    "    \n",
    "    wap = compute_wap(book_stock_time)\n",
    "    returns = log_return(wap)\n",
    "    volatility = realized_volatility(returns)\n",
    "    \n",
    "    return volatility\n",
    "\n",
    "def realized_volatility_per_time_id(file_path, prediction_column_name):\n",
    "    df_book_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Estimate stock price per time point\n",
    "    df_book_data['wap'] = compute_wap(df_book_data)\n",
    "    \n",
    "    # Compute log return from wap values per time_id\n",
    "    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
    "    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n",
    "    \n",
    "    # Compute the square root of the sum of log return squared to get realized volatility\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n",
    "    \n",
    "    # Formatting\n",
    "    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    \n",
    "    return df_realized_vol_per_stock[['row_id',prediction_column_name]]\n",
    "\n",
    "def past_realized_volatility_per_stock(list_file,prediction_column_name):\n",
    "    df_past_realized = pd.DataFrame()\n",
    "    for file in list_file:\n",
    "        df_past_realized = pd.concat([df_past_realized,\n",
    "                                     realized_volatility_per_time_id(file,prediction_column_name)])\n",
    "    return df_past_realized\n",
    "\n",
    "def stupidForestPrediction(book_path_train,prediction_column_name,train_targets_pd,book_path_test):\n",
    "    naive_predictions_train = past_realized_volatility_per_stock(list_file=book_path_train,prediction_column_name=prediction_column_name)\n",
    "    df_joined_train = train_targets_pd.merge(naive_predictions_train[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "    \n",
    "    X = np.array(df_joined_train['pred']).reshape(-1,1)\n",
    "    y = np.array(df_joined_train['target']).reshape(-1,)\n",
    "\n",
    "    regr = RandomForestRegressor(random_state=0)\n",
    "    regr.fit(X, y)\n",
    "    \n",
    "    naive_predictions_test = past_realized_volatility_per_stock(list_file=book_path_test,prediction_column_name='target')\n",
    "    yhat = regr.predict(np.array(naive_predictions_test['target']).reshape(-1,1))\n",
    "    \n",
    "    updated_predictions = naive_predictions_test.copy()\n",
    "    updated_predictions['target'] = yhat\n",
    "    \n",
    "    return updated_predictions\n",
    "\n",
    "def garch_fit_predict_volatility(returns_series, N=10000):\n",
    "    model = arch_model(returns_series * N, p=1, q=1)\n",
    "    model_fit = model.fit(update_freq=0, disp='off')\n",
    "    yhat = model_fit.forecast(horizon=600, reindex=False)\n",
    "\n",
    "    pred_volatility = np.sqrt(np.sum(yhat.variance.values)) / N\n",
    "\n",
    "    return pred_volatility\n",
    "\n",
    "def garch_volatility_per_time_id(file_path, prediction_column_name):\n",
    "    # read the data\n",
    "    df_book_data = pd.read_parquet(file_path) \n",
    "\n",
    "    # calculate the midprice (not the WAP)  \n",
    "    df_book_data['midprice'] =(df_book_data['bid_price1'] + df_book_data['ask_price1'])/2\n",
    "\n",
    "    # leave only WAP for now\n",
    "    df_book_data = df_book_data[['time_id', 'seconds_in_bucket', 'midprice']]\n",
    "    df_book_data = df_book_data.sort_values('seconds_in_bucket')\n",
    "\n",
    "    # make the book updates evenly spaced\n",
    "    df_book_data_evenly = pd.DataFrame({'time_id':np.repeat(df_book_data['time_id'].unique(), 600), \n",
    "                                        'second':np.tile(range(0,600), df_book_data['time_id'].nunique())})\n",
    "    df_book_data_evenly['second'] = df_book_data_evenly['second'].astype(np.int16)\n",
    "    df_book_data_evenly = df_book_data_evenly.sort_values('second')\n",
    "\n",
    "\n",
    "    df_book_data_evenly = pd.merge_asof(df_book_data_evenly,\n",
    "                           df_book_data,\n",
    "                           left_on='second',right_on='seconds_in_bucket',\n",
    "                           by = 'time_id')\n",
    "\n",
    "    # Ordering for easier use\n",
    "    df_book_data_evenly = df_book_data_evenly[['time_id', 'second', 'midprice']]\n",
    "    df_book_data_evenly = df_book_data_evenly.sort_values(['time_id','second']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # calculate log returns \n",
    "    df_book_data_evenly['log_return'] = df_book_data_evenly.groupby(['time_id'])['midprice'].apply(log_return)\n",
    "    df_book_data_evenly = df_book_data_evenly[~df_book_data_evenly['log_return'].isnull()]\n",
    "\n",
    "\n",
    "    # fit GARCH(1, 1) and predict the volatility of returns\n",
    "    df_garch_vol_per_stock =  \\\n",
    "        pd.DataFrame(df_book_data_evenly.groupby(['time_id'])['log_return'].agg(garch_fit_predict_volatility)).reset_index()\n",
    "    df_garch_vol_per_stock = df_garch_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n",
    "\n",
    "    # add row_id column to the data\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_garch_vol_per_stock['row_id'] = df_garch_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "\n",
    "    # return the result\n",
    "    return df_garch_vol_per_stock[['row_id', prediction_column_name]]\n",
    "\n",
    "def garch_volatility_per_stock(list_file, prediction_column_name):\n",
    "    df_garch_predicted = pd.DataFrame()\n",
    "    for file in list_file:\n",
    "        df_garch_predicted = pd.concat([df_garch_predicted,\n",
    "                                     garch_volatility_per_time_id(file, prediction_column_name)])\n",
    "    return df_garch_predicted\n",
    "\n",
    "def entropy_from_book(book_stock_time,last_min):\n",
    "    \n",
    "    if last_min < 10:\n",
    "        book_stock_time = book_stock_time[book_stock_time['seconds_in_bucket'] >= (600-last_min*60)]\n",
    "        if book_stock_time.empty == True or book_stock_time.shape[0] < 3:\n",
    "            return 0\n",
    "        \n",
    "    wap = compute_wap(book_stock_time)\n",
    "    t_init = book_stock_time['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, wap, kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def entropy_from_wap(wap,seconds,last_seconds):\n",
    "    \n",
    "    if last_seconds < 600:\n",
    "        idx = np.where(seconds >= last_seconds)[0]\n",
    "        if len(idx) < 3:\n",
    "            return 0\n",
    "        else:\n",
    "            wap = wap[idx]\n",
    "            seconds = seconds[idx]\n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    t_new = np.arange(np.min(seconds),np.max(seconds))\n",
    "    nearest = interp1d(seconds, wap, kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    # sampleEntropy = ApEn_new(resampled_wap,3,0.001)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def linearFit(book_stock_time, last_min):\n",
    "    \n",
    "    if last_min < 10:\n",
    "        book_stock_time = book_stock_time[book_stock_time['seconds_in_bucket'] >= (600-last_min*60)]\n",
    "        if book_stock_time.empty == True or book_stock_time.shape[0] < 2:\n",
    "            return 0\n",
    "        \n",
    "    wap = np.array(compute_wap(book_stock_time))\n",
    "    t_init = book_stock_time['seconds_in_bucket']\n",
    "    \n",
    "    return (wap[-1] - wap[0])/(np.max(t_init) - np.min(t_init))\n",
    "\n",
    "def wapStat(book_stock_time, last_min):\n",
    "    \n",
    "    if last_min < 10:\n",
    "        book_stock_time = book_stock_time[book_stock_time['seconds_in_bucket'] >= (600-last_min*60)]\n",
    "        if book_stock_time.empty == True or book_stock_time.shape[0] < 2:\n",
    "            return 0\n",
    "        \n",
    "    wap = compute_wap(book_stock_time)\n",
    "    t_init = book_stock_time['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, wap, kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    return np.std(resampled_wap)\n",
    "\n",
    "\n",
    "def entropy_Prediction(book_path_train,prediction_column_name,train_targets_pd,book_path_test,all_stocks_ids,test_file):\n",
    "    \n",
    "    # Compute features\n",
    "    book_features_encoded_test = computeFeatures_1(book_path_test,'test',test_file,all_stocks_ids) \n",
    "    \n",
    "    book_features_encoded_train = computeFeatures_1(book_path_train,'train',train_targets_pd,all_stocks_ids)\n",
    "    \n",
    "    X = book_features_encoded_train.drop(['row_id','target','stock_id'],axis=1)\n",
    "    y = book_features_encoded_train['target']\n",
    "    \n",
    "    # Modeling\n",
    "    catboost_default = CatBoostRegressor(verbose=0)\n",
    "    catboost_default.fit(X,y)\n",
    "    \n",
    "    # Predict\n",
    "    X_test = book_features_encoded_test.drop(['row_id','stock_id'],axis=1)\n",
    "    yhat = catboost_default.predict(X_test)\n",
    "    \n",
    "    # Formatting\n",
    "    yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "    predictions = pd.concat([test_file,yhat_pd],axis=1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def computeFeatures_1(book_path,prediction_column_name,train_targets_pd,all_stocks_ids):\n",
    "    \n",
    "    book_all_features = pd.DataFrame()\n",
    "    encoder = np.eye(len(all_stocks_ids))\n",
    "\n",
    "    stocks_id_list, row_id_list = [], []\n",
    "    volatility_list, entropy2_list = [], []\n",
    "    linearFit_list, linearFit5_list, linearFit2_list = [], [], []\n",
    "    wap_std_list, wap_std5_list, wap_std2_list = [], [], []\n",
    "\n",
    "    for file in book_path:\n",
    "        start = time.time()\n",
    "\n",
    "        book_stock = pd.read_parquet(file)\n",
    "        stock_id = file.split('=')[1]\n",
    "        print('stock id computing = ' + str(stock_id))\n",
    "        stock_time_ids = book_stock['time_id'].unique()\n",
    "        for time_id in stock_time_ids:     \n",
    "\n",
    "            # Access book data at this time + stock\n",
    "            book_stock_time = book_stock[book_stock['time_id'] == time_id]\n",
    "\n",
    "            # Create feature matrix\n",
    "            stocks_id_list.append(stock_id)\n",
    "            row_id_list.append(str(f'{stock_id}-{time_id}'))\n",
    "            volatility_list.append(realized_volatility_from_book_pd(book_stock_time=book_stock_time))\n",
    "            entropy2_list.append(entropy_from_book(book_stock_time=book_stock_time,last_min=2))\n",
    "            linearFit_list.append(linearFit(book_stock_time=book_stock_time,last_min=10))\n",
    "            linearFit5_list.append(linearFit(book_stock_time=book_stock_time,last_min=5))\n",
    "            linearFit2_list.append(linearFit(book_stock_time=book_stock_time,last_min=2))\n",
    "            wap_std_list.append(wapStat(book_stock_time=book_stock_time,last_min=10))\n",
    "            wap_std5_list.append(wapStat(book_stock_time=book_stock_time,last_min=5))\n",
    "            wap_std2_list.append(wapStat(book_stock_time=book_stock_time,last_min=2))\n",
    "\n",
    "        print('Computing one stock entropy took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Merge targets\n",
    "    stocks_id_pd = pd.DataFrame(stocks_id_list,columns=['stock_id'])\n",
    "    row_id_pd = pd.DataFrame(row_id_list,columns=['row_id'])\n",
    "    volatility_pd = pd.DataFrame(volatility_list,columns=['volatility'])\n",
    "    entropy2_pd = pd.DataFrame(entropy2_list,columns=['entropy2'])\n",
    "    linearFit_pd = pd.DataFrame(linearFit_list,columns=['linearFit_coef'])\n",
    "    linearFit5_pd = pd.DataFrame(linearFit5_list,columns=['linearFit_coef5'])\n",
    "    linearFit2_pd = pd.DataFrame(linearFit2_list,columns=['linearFit_coef2'])\n",
    "    wap_std_pd = pd.DataFrame(wap_std_list,columns=['wap_std'])\n",
    "    wap_std5_pd = pd.DataFrame(wap_std5_list,columns=['wap_std5'])\n",
    "    wap_std2_pd = pd.DataFrame(wap_std2_list,columns=['wap_std2'])\n",
    "\n",
    "    book_all_features = pd.concat([stocks_id_pd,row_id_pd,volatility_pd,entropy2_pd,linearFit_pd,linearFit5_pd,linearFit2_pd,\n",
    "                                  wap_std_pd,wap_std5_pd,wap_std2_pd],axis=1)\n",
    "\n",
    "    # This line makes sure the predictions are aligned with the row_id in the submission file\n",
    "    book_all_features = train_targets_pd.merge(book_all_features, on = ['row_id'])\n",
    "\n",
    "    # Add encoded stock\n",
    "    encoded = list()\n",
    "\n",
    "    for i in range(book_all_features.shape[0]):\n",
    "        stock_id = book_all_features['stock_id'][i]\n",
    "        encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]\n",
    "        encoded.append(encoded_stock)\n",
    "\n",
    "    encoded_pd = pd.DataFrame(np.array(encoded).reshape(book_all_features.shape[0],np.array(all_stocks_ids).shape[0]))\n",
    "    book_all_features_encoded = pd.concat([book_all_features, encoded_pd],axis=1)\n",
    "    \n",
    "    return book_all_features_encoded\n",
    "\n",
    "def calc_wap(df):\n",
    "    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def calc_wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def calc_wap3(df):\n",
    "    return (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def calc_wap4(df):\n",
    "    return (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def mid_price(df):\n",
    "    return ((df['bid_price1'] + df['ask_price1']) / 2)\n",
    "\n",
    "def calc_rv_from_wap_numba(values, index):\n",
    "    log_return = np.diff(np.log(values))\n",
    "    realized_vol = np.sqrt(np.sum(np.square(log_return[1:])))\n",
    "    return realized_vol\n",
    "\n",
    "def load_book_data_by_id(stock_id,datapath,train_test):\n",
    "    file_to_read = os.path.join(datapath,'book_' + train_test + str('.parquet'),'stock_id=' + str(stock_id))\n",
    "    df = pd.read_parquet(file_to_read)\n",
    "    return df\n",
    "\n",
    "def load_trade_data_by_id(stock_id,datapath,train_test):\n",
    "    file_to_read = os.path.join(datapath,'trade_' + str(train_test) + str('.parquet'),'stock_id=' + str(stock_id))\n",
    "    df = pd.read_parquet(file_to_read)\n",
    "    return df\n",
    "\n",
    "def load_trade_data_by_id_kaggle(stock_id,train_test):\n",
    "    if train_test == 'train':\n",
    "        input_file = f'/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id={stock_id}'\n",
    "    elif train_test == 'test':\n",
    "        input_file = f'/kaggle/input/optiver-realized-volatility-prediction/trade_test.parquet/stock_id={stock_id}'\n",
    "    df = pd.read_parquet(input_file)\n",
    "    return df\n",
    "\n",
    "def entropy_from_df(df):\n",
    "    \n",
    "    if df.shape[0] < 3:\n",
    "        return 0\n",
    "        \n",
    "    t_init = df['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, df['wap'], kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def entropy_from_df2(df):\n",
    "    \n",
    "    if df.shape[0] < 3:\n",
    "        return 0\n",
    "        \n",
    "    t_init = df['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, df['wap2'], kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def entropy_from_df3(df):\n",
    "    \n",
    "    if df.shape[0] < 3:\n",
    "        return 0\n",
    "        \n",
    "    t_init = df['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, df['wap3'], kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def financial_metrics(df):\n",
    "    \n",
    "    wap_imbalance = np.mean(df['wap'] - df['wap2'])\n",
    "    price_spread = np.mean((df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2))\n",
    "    bid_spread = np.mean(df['bid_price1'] - df['bid_price2'])  \n",
    "    ask_spread = np.mean(df['ask_price1'] - df['ask_price2']) # Abs to take\n",
    "    total_volume = np.mean((df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2']))\n",
    "    volume_imbalance = np.mean(abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2'])))\n",
    "    \n",
    "    return [wap_imbalance,price_spread,bid_spread,ask_spread,total_volume,volume_imbalance]\n",
    "\n",
    "def financial_metrics_2(df):\n",
    "    \n",
    "    wap_imbalance = np.mean(df['wap'] - df['wap2'])\n",
    "    price_spread = np.mean((df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2))\n",
    "    bid_spread = np.mean(df['bid_price1'] - df['bid_price2'])  \n",
    "    ask_spread = np.mean(df['ask_price1'] - df['ask_price2']) # Abs to take\n",
    "    total_volume = np.mean((df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2']))\n",
    "    volume_imbalance = np.mean(abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2'])))\n",
    "    \n",
    "    # New features here\n",
    "    \n",
    "    return [wap_imbalance,price_spread,bid_spread,ask_spread,total_volume,volume_imbalance]\n",
    "\n",
    "def other_metrics(df):\n",
    "    \n",
    "    if df.shape[0] < 2:\n",
    "        linearFit = 0\n",
    "        linearFit2 = 0\n",
    "        linearFit3 = 0\n",
    "        std_1 = 0\n",
    "        std_2 = 0\n",
    "        std_3 = 0\n",
    "    else:\n",
    "        linearFit = (df['wap'].iloc[-1] - df['wap'].iloc[0]) / ((np.max(df['seconds_in_bucket']) - np.min(df['seconds_in_bucket']))) \n",
    "        linearFit2 = (df['wap2'].iloc[-1] - df['wap2'].iloc[0]) / ((np.max(df['seconds_in_bucket']) - np.min(df['seconds_in_bucket']))) \n",
    "        linearFit3 = (df['wap3'].iloc[-1] - df['wap3'].iloc[0]) / ((np.max(df['seconds_in_bucket']) - np.min(df['seconds_in_bucket']))) \n",
    "    \n",
    "        # Resampling\n",
    "        t_init = df['seconds_in_bucket']\n",
    "        t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "\n",
    "        # Closest neighbour interpolation (no changes in wap between lines)\n",
    "        nearest = interp1d(t_init, df['wap'], kind='nearest')\n",
    "        nearest2 = interp1d(t_init, df['wap2'], kind='nearest')\n",
    "        nearest3 = interp1d(t_init, df['wap3'], kind='nearest')\n",
    "\n",
    "        std_1 = np.std(nearest(t_new))\n",
    "        std_2 = np.std(nearest2(t_new))\n",
    "        std_3 = np.std(nearest3(t_new))\n",
    "    \n",
    "    return [linearFit, linearFit2, linearFit3, std_1, std_2, std_3]\n",
    "\n",
    "def load_book_data_by_id_kaggle(stock_id,train_test):\n",
    "    df = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/book_{train_test}.parquet/stock_id={stock_id}')\n",
    "    return df\n",
    "\n",
    "\n",
    "def computeFeatures_wEntropy(machine, dataset, all_stocks_ids, datapath):\n",
    "    \n",
    "    list_rv, list_rv2, list_rv3 = [], [], []\n",
    "    list_ent, list_fin, list_fin2 = [], [], []\n",
    "    list_others, list_others2, list_others3 = [], [], []\n",
    "\n",
    "    for stock_id in range(127):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if machine == 'local':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id(stock_id,datapath,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        elif machine == 'kaggle':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id_kaggle(stock_id,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Useful\n",
    "        all_time_ids_byStock = book_stock['time_id'].unique() \n",
    "\n",
    "        # Calculate wap for the book\n",
    "        book_stock['wap'] = calc_wap(book_stock)\n",
    "        book_stock['wap2'] = calc_wap2(book_stock)\n",
    "        book_stock['wap3'] = calc_wap3(book_stock)\n",
    "\n",
    "        # Calculate realized volatility\n",
    "        df_sub = book_stock.groupby('time_id')['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub2 = book_stock.groupby('time_id')['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub3 = book_stock.groupby('time_id')['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub['time_id']]\n",
    "        df_sub = pd.concat([df_sub,df_sub2['wap2'],df_sub3['wap3']],axis=1)\n",
    "        df_sub = df_sub.rename(columns={'time_id':'row_id','wap': 'rv', 'wap2': 'rv2', 'wap3': 'rv3'})\n",
    "        \n",
    "        # Calculate realized volatility last 5 min\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 300').empty\n",
    "        if isEmpty == False:\n",
    "            df_sub_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub_5['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_5['time_id']]\n",
    "            df_sub_5 = pd.concat([df_sub_5,df_sub2_5['wap2'],df_sub3_5['wap3']],axis=1)\n",
    "            df_sub_5 = df_sub_5.rename(columns={'time_id':'row_id','wap': 'rv_5', 'wap2': 'rv2_5', 'wap3': 'rv3_5'})\n",
    "        else: # 0 volatility\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_5'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_5'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_5'])\n",
    "            df_sub_5 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3],axis=1) \n",
    "\n",
    "        # Calculate realized volatility last 2 min\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 480').empty\n",
    "        if isEmpty == False:\n",
    "            df_sub_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()    \n",
    "            df_sub_2['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_2['time_id']] \n",
    "            df_sub_2 = pd.concat([df_sub_2,df_sub2_2['wap2'],df_sub3_2['wap3']],axis=1)\n",
    "            df_sub_2 = df_sub_2.rename(columns={'time_id':'row_id','wap': 'rv_2', 'wap2': 'rv2_2', 'wap3': 'rv3_2'})\n",
    "        else: # 0 volatility\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_2'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_2'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_2'])\n",
    "            df_sub_2 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3],axis=1) \n",
    "\n",
    "        list_rv.append(df_sub)\n",
    "        list_rv2.append(df_sub_5)\n",
    "        list_rv3.append(df_sub_2)\n",
    "\n",
    "        # Calculate other financial metrics from book \n",
    "        df_sub_book_feats = book_stock.groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={0:'embedding'})\n",
    "        df_sub_book_feats[['wap_imbalance','price_spread','bid_spread','ask_spread','total_vol','vol_imbalance']] = pd.DataFrame(df_sub_book_feats.embedding.tolist(), index=df_sub_book_feats.index)\n",
    "        df_sub_book_feats['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats['time_id']] \n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 300').empty\n",
    "        if isEmpty == False:\n",
    "            df_sub_book_feats5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "            df_sub_book_feats5 = df_sub_book_feats5.rename(columns={0:'embedding'})\n",
    "            df_sub_book_feats5[['wap_imbalance5','price_spread5','bid_spread5','ask_spread5','total_vol5','vol_imbalance5']] = pd.DataFrame(df_sub_book_feats5.embedding.tolist(), index=df_sub_book_feats5.index)\n",
    "            df_sub_book_feats5['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats5['time_id']] \n",
    "            df_sub_book_feats5 = df_sub_book_feats5.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['wap_imbalance5']) \n",
    "            temp2 = pd.DataFrame([0],columns=['price_spread5'])\n",
    "            temp3 = pd.DataFrame([0],columns=['bid_spread5'])\n",
    "            temp4 = pd.DataFrame([0],columns=['ask_spread5'])\n",
    "            temp5 = pd.DataFrame([0],columns=['total_vol5'])\n",
    "            temp6 = pd.DataFrame([0],columns=['vol_imbalance5'])\n",
    "            df_sub_book_feats5 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1) \n",
    "            \n",
    "        list_fin.append(df_sub_book_feats)\n",
    "        list_fin2.append(df_sub_book_feats5)\n",
    "\n",
    "        # Compute entropy \n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 480').empty\n",
    "        if isEmpty == False:\n",
    "            df_ent = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(entropy_from_df).to_frame().reset_index().fillna(0)\n",
    "            df_ent2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(entropy_from_df2).to_frame().reset_index().fillna(0)\n",
    "            df_ent3 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(entropy_from_df3).to_frame().reset_index().fillna(0)\n",
    "            df_ent['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_ent['time_id']]\n",
    "            df_ent = df_ent.rename(columns={'time_id':'row_id',0:'entropy'})\n",
    "            df_ent2 = df_ent2.rename(columns={0:'entropy2'}).drop(['time_id'],axis=1)\n",
    "            df_ent3 = df_ent3.rename(columns={0:'entropy3'}).drop(['time_id'],axis=1)\n",
    "            df_ent = pd.concat([df_ent,df_ent2,df_ent3],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['entropy']) \n",
    "            temp2 = pd.DataFrame([0],columns=['entropy2'])\n",
    "            temp3 = pd.DataFrame([0],columns=['entropy3'])\n",
    "            df_ent = pd.concat([times_pd,temp,temp2,temp3],axis=1)\n",
    "            \n",
    "        list_ent.append(df_ent)\n",
    "\n",
    "        # Compute other metrics\n",
    "        df_others = book_stock.groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "        df_others = df_others.rename(columns={0:'embedding'})\n",
    "        df_others[['linearFit1_1','linearFit1_2','linearFit1_3','wap_std1_1','wap_std1_2','wap_std1_3']] = pd.DataFrame(df_others.embedding.tolist(), index=df_others.index)\n",
    "        df_others['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others['time_id']] \n",
    "        df_others = df_others.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        list_others.append(df_others)\n",
    "\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 300').empty\n",
    "        if isEmpty == False:\n",
    "            df_others2 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "            df_others2 = df_others2.rename(columns={0:'embedding'})\n",
    "            df_others2[['linearFit2_1','linearFit2_2','linearFit2_3','wap_std2_1','wap_std2_2','wap_std2_3']] = pd.DataFrame(df_others2.embedding.tolist(), index=df_others2.index)\n",
    "            df_others2['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others2['time_id']] \n",
    "            df_others2 = df_others2.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['linearFit2_1']) \n",
    "            temp2 = pd.DataFrame([0],columns=['linearFit2_2'])\n",
    "            temp3 = pd.DataFrame([0],columns=['linearFit2_3'])\n",
    "            temp4 = pd.DataFrame([0],columns=['wap_std2_1'])\n",
    "            temp5 = pd.DataFrame([0],columns=['wap_std2_2'])\n",
    "            temp6 = pd.DataFrame([0],columns=['wap_std2_3'])\n",
    "            df_others2 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1)\n",
    "            \n",
    "        list_others2.append(df_others2)\n",
    "\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 480').empty \n",
    "        if isEmpty == False:\n",
    "            df_others3 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "            df_others3 = df_others3.rename(columns={0:'embedding'})\n",
    "            df_others3[['linearFit3_1','linearFit3_2','linearFit3_3','wap_std3_1','wap_std3_2','wap_std3_3']] = pd.DataFrame(df_others3.embedding.tolist(), index=df_others3.index)\n",
    "            df_others3['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others3['time_id']] \n",
    "            df_others3 = df_others3.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['linearFit3_1']) \n",
    "            temp2 = pd.DataFrame([0],columns=['linearFit3_2'])\n",
    "            temp3 = pd.DataFrame([0],columns=['linearFit3_3'])\n",
    "            temp4 = pd.DataFrame([0],columns=['wap_std3_1'])\n",
    "            temp5 = pd.DataFrame([0],columns=['wap_std3_2'])\n",
    "            temp6 = pd.DataFrame([0],columns=['wap_std3_3'])\n",
    "            df_others3 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1)\n",
    "            \n",
    "        list_others3.append(df_others3)\n",
    "\n",
    "        print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Create features dataframe\n",
    "    df_submission = pd.concat(list_rv)\n",
    "    df_submission2 = pd.concat(list_rv2)\n",
    "    df_submission3 = pd.concat(list_rv3)\n",
    "    df_ent_concat = pd.concat(list_ent)\n",
    "    df_fin_concat = pd.concat(list_fin)\n",
    "    df_fin2_concat = pd.concat(list_fin2)\n",
    "    df_others = pd.concat(list_others)\n",
    "    df_others2 = pd.concat(list_others2)\n",
    "    df_others3 = pd.concat(list_others3)\n",
    "\n",
    "    df_book_features = df_submission.merge(df_submission2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_submission3, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_ent_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin2_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_others, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_others2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_others3, on = ['row_id'], how='left').fillna(0)\n",
    "    \n",
    "    # Add encoded stock\n",
    "    encoder = np.eye(len(all_stocks_ids))\n",
    "    encoded = list()\n",
    "\n",
    "    for i in range(df_book_features.shape[0]):\n",
    "        stock_id = int(df_book_features['row_id'][i].split('-')[0])\n",
    "        encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]\n",
    "        encoded.append(encoded_stock)\n",
    "\n",
    "    encoded_pd = pd.DataFrame(np.array(encoded).reshape(df_book_features.shape[0],np.array(all_stocks_ids).shape[0]))\n",
    "    df_book_features_encoded = pd.concat([df_book_features, encoded_pd],axis=1)\n",
    "    \n",
    "    return df_book_features_encoded\n",
    "\n",
    "def computeFeatures_july(machine, dataset, all_stocks_ids, datapath):\n",
    "    \n",
    "    list_rv, list_rv2, list_rv3 = [], [], []\n",
    "    list_ent, list_fin, list_fin2 = [], [], []\n",
    "    list_others, list_others2, list_others3 = [], [], []\n",
    "\n",
    "    for stock_id in range(127):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if machine == 'local':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id(stock_id,datapath,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        elif machine == 'kaggle':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id_kaggle(stock_id,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Useful\n",
    "        all_time_ids_byStock = book_stock['time_id'].unique() \n",
    "\n",
    "        # Calculate wap for the book\n",
    "        book_stock['wap'] = calc_wap(book_stock)\n",
    "        book_stock['wap2'] = calc_wap2(book_stock)\n",
    "        book_stock['wap3'] = calc_wap3(book_stock)\n",
    "\n",
    "        # Calculate realized volatility\n",
    "        df_sub = book_stock.groupby('time_id')['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub2 = book_stock.groupby('time_id')['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub3 = book_stock.groupby('time_id')['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub['time_id']]\n",
    "        df_sub = pd.concat([df_sub,df_sub2['wap2'],df_sub3['wap3']],axis=1)\n",
    "        df_sub = df_sub.rename(columns={'time_id':'row_id','wap': 'rv', 'wap2': 'rv2', 'wap3': 'rv3'})\n",
    "        \n",
    "        # Calculate realized volatility last 5 min\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 300').empty\n",
    "        if isEmpty == False:\n",
    "            df_sub_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub_5['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_5['time_id']]\n",
    "            df_sub_5 = pd.concat([df_sub_5,df_sub2_5['wap2'],df_sub3_5['wap3']],axis=1)\n",
    "            df_sub_5 = df_sub_5.rename(columns={'time_id':'row_id','wap': 'rv_5', 'wap2': 'rv2_5', 'wap3': 'rv3_5'})\n",
    "        else: # 0 volatility\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_5'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_5'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_5'])\n",
    "            df_sub_5 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3],axis=1) \n",
    "\n",
    "        # Calculate realized volatility last 2 min\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 480').empty\n",
    "        if isEmpty == False:\n",
    "            df_sub_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()    \n",
    "            df_sub_2['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_2['time_id']] \n",
    "            df_sub_2 = pd.concat([df_sub_2,df_sub2_2['wap2'],df_sub3_2['wap3']],axis=1)\n",
    "            df_sub_2 = df_sub_2.rename(columns={'time_id':'row_id','wap': 'rv_2', 'wap2': 'rv2_2', 'wap3': 'rv3_2'})\n",
    "        else: # 0 volatility\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_2'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_2'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_2'])\n",
    "            df_sub_2 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3],axis=1) \n",
    "\n",
    "        list_rv.append(df_sub)\n",
    "        list_rv2.append(df_sub_5)\n",
    "        list_rv3.append(df_sub_2)\n",
    "\n",
    "        # Calculate other financial metrics from book \n",
    "        df_sub_book_feats = book_stock.groupby(['time_id']).apply(financial_metrics_2).to_frame().reset_index()\n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={0:'embedding'})\n",
    "        df_sub_book_feats[['wap_imbalance','price_spread','bid_spread','ask_spread','total_vol','vol_imbalance']] = pd.DataFrame(df_sub_book_feats.embedding.tolist(), index=df_sub_book_feats.index)\n",
    "        df_sub_book_feats['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats['time_id']] \n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 300').empty\n",
    "        if isEmpty == False:\n",
    "            df_sub_book_feats5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(financial_metrics_2).to_frame().reset_index()\n",
    "            df_sub_book_feats5 = df_sub_book_feats5.rename(columns={0:'embedding'})\n",
    "            df_sub_book_feats5[['wap_imbalance5','price_spread5','bid_spread5','ask_spread5','total_vol5','vol_imbalance5']] = pd.DataFrame(df_sub_book_feats5.embedding.tolist(), index=df_sub_book_feats5.index)\n",
    "            df_sub_book_feats5['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats5['time_id']] \n",
    "            df_sub_book_feats5 = df_sub_book_feats5.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['wap_imbalance5']) \n",
    "            temp2 = pd.DataFrame([0],columns=['price_spread5'])\n",
    "            temp3 = pd.DataFrame([0],columns=['bid_spread5'])\n",
    "            temp4 = pd.DataFrame([0],columns=['ask_spread5'])\n",
    "            temp5 = pd.DataFrame([0],columns=['total_vol5'])\n",
    "            temp6 = pd.DataFrame([0],columns=['vol_imbalance5'])\n",
    "            df_sub_book_feats5 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1) \n",
    "            \n",
    "        list_fin.append(df_sub_book_feats)\n",
    "        list_fin2.append(df_sub_book_feats5)\n",
    "\n",
    "        # Compute other metrics\n",
    "        df_others = book_stock.groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "        df_others = df_others.rename(columns={0:'embedding'})\n",
    "        df_others[['linearFit1_1','linearFit1_2','linearFit1_3','wap_std1_1','wap_std1_2','wap_std1_3']] = pd.DataFrame(df_others.embedding.tolist(), index=df_others.index)\n",
    "        df_others['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others['time_id']] \n",
    "        df_others = df_others.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        list_others.append(df_others)\n",
    "\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 300').empty\n",
    "        if isEmpty == False:\n",
    "            df_others2 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "            df_others2 = df_others2.rename(columns={0:'embedding'})\n",
    "            df_others2[['linearFit2_1','linearFit2_2','linearFit2_3','wap_std2_1','wap_std2_2','wap_std2_3']] = pd.DataFrame(df_others2.embedding.tolist(), index=df_others2.index)\n",
    "            df_others2['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others2['time_id']] \n",
    "            df_others2 = df_others2.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['linearFit2_1']) \n",
    "            temp2 = pd.DataFrame([0],columns=['linearFit2_2'])\n",
    "            temp3 = pd.DataFrame([0],columns=['linearFit2_3'])\n",
    "            temp4 = pd.DataFrame([0],columns=['wap_std2_1'])\n",
    "            temp5 = pd.DataFrame([0],columns=['wap_std2_2'])\n",
    "            temp6 = pd.DataFrame([0],columns=['wap_std2_3'])\n",
    "            df_others2 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1)\n",
    "            \n",
    "        list_others2.append(df_others2)\n",
    "\n",
    "        isEmpty = book_stock.query(f'seconds_in_bucket >= 480').empty \n",
    "        if isEmpty == False:\n",
    "            df_others3 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "            df_others3 = df_others3.rename(columns={0:'embedding'})\n",
    "            df_others3[['linearFit3_1','linearFit3_2','linearFit3_3','wap_std3_1','wap_std3_2','wap_std3_3']] = pd.DataFrame(df_others3.embedding.tolist(), index=df_others3.index)\n",
    "            df_others3['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others3['time_id']] \n",
    "            df_others3 = df_others3.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['linearFit3_1']) \n",
    "            temp2 = pd.DataFrame([0],columns=['linearFit3_2'])\n",
    "            temp3 = pd.DataFrame([0],columns=['linearFit3_3'])\n",
    "            temp4 = pd.DataFrame([0],columns=['wap_std3_1'])\n",
    "            temp5 = pd.DataFrame([0],columns=['wap_std3_2'])\n",
    "            temp6 = pd.DataFrame([0],columns=['wap_std3_3'])\n",
    "            df_others3 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1)\n",
    "            \n",
    "        list_others3.append(df_others3)\n",
    "\n",
    "        print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Create features dataframe\n",
    "    df_submission = pd.concat(list_rv)\n",
    "    df_submission2 = pd.concat(list_rv2)\n",
    "    df_submission3 = pd.concat(list_rv3)\n",
    "    df_ent_concat = pd.concat(list_ent)\n",
    "    df_fin_concat = pd.concat(list_fin)\n",
    "    df_fin2_concat = pd.concat(list_fin2)\n",
    "    df_others = pd.concat(list_others)\n",
    "    df_others2 = pd.concat(list_others2)\n",
    "    df_others3 = pd.concat(list_others3)\n",
    "\n",
    "    df_book_features = df_submission.merge(df_submission2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_submission3, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_ent_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin2_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_others, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_others2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_others3, on = ['row_id'], how='left').fillna(0)\n",
    "    \n",
    "    # Add encoded stock\n",
    "    encoder = np.eye(len(all_stocks_ids))\n",
    "    encoded = list()\n",
    "\n",
    "    for i in range(df_book_features.shape[0]):\n",
    "        stock_id = int(df_book_features['row_id'][i].split('-')[0])\n",
    "        encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]\n",
    "        encoded.append(encoded_stock)\n",
    "\n",
    "    encoded_pd = pd.DataFrame(np.array(encoded).reshape(df_book_features.shape[0],np.array(all_stocks_ids).shape[0]))\n",
    "    df_book_features_encoded = pd.concat([df_book_features, encoded_pd],axis=1)\n",
    "    \n",
    "    return df_book_features_encoded\n",
    "\n",
    "def computeFeatures_newTest_Laurent(machine, dataset, all_stocks_ids, datapath):\n",
    "    \n",
    "    list_rv, list_rv2, list_rv3 = [], [], []\n",
    "    list_ent, list_fin, list_fin2 = [], [], []\n",
    "    list_others, list_others2, list_others3 = [], [], []\n",
    "\n",
    "    for stock_id in range(127):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if machine == 'local':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id(stock_id,datapath,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        elif machine == 'kaggle':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id_kaggle(stock_id,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Useful\n",
    "        all_time_ids_byStock = book_stock['time_id'].unique() \n",
    "\n",
    "        # Calculate wap for the entire book\n",
    "        book_stock['wap'] = calc_wap(book_stock)\n",
    "        book_stock['wap2'] = calc_wap2(book_stock)\n",
    "        book_stock['wap3'] = calc_wap3(book_stock)\n",
    "        book_stock['wap4'] = calc_wap2(book_stock)\n",
    "        book_stock['mid_price'] = calc_wap3(book_stock)\n",
    "\n",
    "        # Calculate past realized volatility per time_id\n",
    "        df_sub = book_stock.groupby('time_id')['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub2 = book_stock.groupby('time_id')['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub3 = book_stock.groupby('time_id')['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub4 = book_stock.groupby('time_id')['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub5 = book_stock.groupby('time_id')['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        \n",
    "        df_sub['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub['time_id']]\n",
    "        df_sub = df_sub.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        df_sub = pd.concat([df_sub,df_sub2['wap2'],df_sub3['wap3'], df_sub4['wap4'], df_sub5['mid_price']],axis=1)\n",
    "        df_sub = df_sub.rename(columns={'wap': 'rv', 'wap2': 'rv2', 'wap3': 'rv3', 'wap4':'rv4','mid_price':'rv5'})\n",
    "        \n",
    "        list_rv.append(df_sub)\n",
    "        \n",
    "        # Query segments\n",
    "        bucketQuery480 = book_stock.query(f'seconds_in_bucket >= 480')\n",
    "        isEmpty480 = bucketQuery480.empty\n",
    "        \n",
    "        bucketQuery300 = book_stock.query(f'seconds_in_bucket >= 300')\n",
    "        isEmpty300 = bucketQuery300.empty\n",
    "        \n",
    "        times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "        times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "        times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        # Calculate past realized volatility per time_id and query subset\n",
    "        if isEmpty300 == False:\n",
    "            df_sub_300 = bucketQuery300.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_300 = bucketQuery300.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_300 = bucketQuery300.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_300 = bucketQuery300.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_300 = bucketQuery300.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_300 = pd.concat([times_pd,df_sub_300['wap'],df_sub2_300['wap2'],df_sub3_300['wap3'],df_sub4_300['wap4'],df_sub5_300['mid_price']],axis=1)\n",
    "            df_sub_300 = df_sub_300.rename(columns={'wap': 'rv_300', 'wap2_300': 'rv2', 'wap3_300': 'rv3', 'wap4':'rv4_300','mid_price':'rv5_300'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_300'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_300'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_300'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_300'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_300'])\n",
    "            df_sub_300 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "            \n",
    "        list_rv2.append(df_sub_300)\n",
    "        \n",
    "        # Calculate realized volatility last 2 min\n",
    "        if isEmpty480 == False:\n",
    "            df_sub_480 = bucketQuery480.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_480 = bucketQuery480.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_480 = bucketQuery480.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_480 = bucketQuery480.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_480 = bucketQuery480.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_480 = pd.concat([times_pd,df_sub_480['wap'],df_sub2_480['wap2'],df_sub3_480['wap3'],df_sub4_480['wap4'],df_sub5_480['mid_price']],axis=1)\n",
    "            df_sub_480 = df_sub_480.rename(columns={'wap': 'rv_480', 'wap2_480': 'rv2', 'wap3_480': 'rv3', 'wap4':'rv4_480','mid_price':'rv5_480'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_480'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_480'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_480'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_480'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_480'])\n",
    "            df_sub_480 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "\n",
    "        \n",
    "        list_rv3.append(df_sub_480)\n",
    "\n",
    "        # Calculate other financial metrics from book \n",
    "        df_sub_book_feats = book_stock.groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={0:'embedding'})\n",
    "        df_sub_book_feats[['wap_imbalance','price_spread','bid_spread','ask_spread','total_vol','vol_imbalance']] = pd.DataFrame(df_sub_book_feats.embedding.tolist(), index=df_sub_book_feats.index)\n",
    "        df_sub_book_feats['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats['time_id']] \n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "\n",
    "        list_fin.append(df_sub_book_feats)\n",
    "            \n",
    "        if isEmpty300 == False:\n",
    "            df_sub_book_feats_300 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={0:'embedding'})\n",
    "            df_sub_book_feats_300[['wap_imbalance5','price_spread5','bid_spread5','ask_spread5','total_vol5','vol_imbalance5']] = pd.DataFrame(df_sub_book_feats_300.embedding.tolist(), index=df_sub_book_feats_300.index)\n",
    "            df_sub_book_feats_300['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats_300['time_id']] \n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['wap_imbalance5']) \n",
    "            temp2 = pd.DataFrame([0],columns=['price_spread5'])\n",
    "            temp3 = pd.DataFrame([0],columns=['bid_spread5'])\n",
    "            temp4 = pd.DataFrame([0],columns=['ask_spread5'])\n",
    "            temp5 = pd.DataFrame([0],columns=['total_vol5'])\n",
    "            temp6 = pd.DataFrame([0],columns=['vol_imbalance5'])\n",
    "            df_sub_book_feats_300 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1) \n",
    "            \n",
    "        list_fin2.append(df_sub_book_feats_300)\n",
    "        \n",
    "        print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Create features dataframe\n",
    "    df_submission = pd.concat(list_rv)\n",
    "    df_submission2 = pd.concat(list_rv2)\n",
    "    df_submission3 = pd.concat(list_rv3)\n",
    "    df_fin_concat = pd.concat(list_fin)\n",
    "    df_fin2_concat = pd.concat(list_fin2)\n",
    "\n",
    "    df_book_features = df_submission.merge(df_submission2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_submission3, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin2_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    \n",
    "    # Add encoded stock\n",
    "    encoder = np.eye(len(all_stocks_ids))\n",
    "    encoded = list()\n",
    "\n",
    "    for i in range(df_book_features.shape[0]):\n",
    "        stock_id = int(df_book_features['row_id'][i].split('-')[0])\n",
    "        encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]\n",
    "        encoded.append(encoded_stock)\n",
    "\n",
    "    encoded_pd = pd.DataFrame(np.array(encoded).reshape(df_book_features.shape[0],np.array(all_stocks_ids).shape[0]))\n",
    "    df_book_features_encoded = pd.concat([df_book_features, encoded_pd],axis=1)\n",
    "    \n",
    "    return df_book_features_encoded\n",
    "\n",
    "def computeFeatures_newTest_Laurent_noCode(machine, dataset, all_stocks_ids, datapath):\n",
    "    \n",
    "    list_rv, list_rv2, list_rv3 = [], [], []\n",
    "    list_ent, list_fin, list_fin2 = [], [], []\n",
    "    list_others, list_others2, list_others3 = [], [], []\n",
    "\n",
    "    for stock_id in range(127):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if machine == 'local':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id(stock_id,datapath,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        elif machine == 'kaggle':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id_kaggle(stock_id,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Useful\n",
    "        all_time_ids_byStock = book_stock['time_id'].unique() \n",
    "\n",
    "        # Calculate wap for the entire book\n",
    "        book_stock['wap'] = calc_wap(book_stock)\n",
    "        book_stock['wap2'] = calc_wap2(book_stock)\n",
    "        book_stock['wap3'] = calc_wap3(book_stock)\n",
    "        book_stock['wap4'] = calc_wap2(book_stock)\n",
    "        book_stock['mid_price'] = calc_wap3(book_stock)\n",
    "\n",
    "        # Calculate past realized volatility per time_id\n",
    "        df_sub = book_stock.groupby('time_id')['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub2 = book_stock.groupby('time_id')['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub3 = book_stock.groupby('time_id')['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub4 = book_stock.groupby('time_id')['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub5 = book_stock.groupby('time_id')['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        \n",
    "        df_sub['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub['time_id']]\n",
    "        df_sub = df_sub.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        df_sub = pd.concat([df_sub,df_sub2['wap2'],df_sub3['wap3'], df_sub4['wap4'], df_sub5['mid_price']],axis=1)\n",
    "        df_sub = df_sub.rename(columns={'wap': 'rv', 'wap2': 'rv2', 'wap3': 'rv3', 'wap4':'rv4','mid_price':'rv5'})\n",
    "        \n",
    "        list_rv.append(df_sub)\n",
    "        \n",
    "        # Query segments\n",
    "        bucketQuery480 = book_stock.query(f'seconds_in_bucket >= 480')\n",
    "        isEmpty480 = bucketQuery480.empty\n",
    "        \n",
    "        bucketQuery300 = book_stock.query(f'seconds_in_bucket >= 300')\n",
    "        isEmpty300 = bucketQuery300.empty\n",
    "        \n",
    "        times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "        times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "        times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        # Calculate past realized volatility per time_id and query subset\n",
    "        if isEmpty300 == False:\n",
    "            df_sub_300 = bucketQuery300.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_300 = bucketQuery300.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_300 = bucketQuery300.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_300 = bucketQuery300.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_300 = bucketQuery300.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_300 = pd.concat([times_pd,df_sub_300['wap'],df_sub2_300['wap2'],df_sub3_300['wap3'],df_sub4_300['wap4'],df_sub5_300['mid_price']],axis=1)\n",
    "            df_sub_300 = df_sub_300.rename(columns={'wap': 'rv_300', 'wap2_300': 'rv2', 'wap3_300': 'rv3', 'wap4':'rv4_300','mid_price':'rv5_300'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_300'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_300'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_300'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_300'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_300'])\n",
    "            df_sub_300 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "            \n",
    "        list_rv2.append(df_sub_300)\n",
    "        \n",
    "        # Calculate realized volatility last 2 min\n",
    "        if isEmpty480 == False:\n",
    "            df_sub_480 = bucketQuery480.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_480 = bucketQuery480.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_480 = bucketQuery480.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_480 = bucketQuery480.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_480 = bucketQuery480.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_480 = pd.concat([times_pd,df_sub_480['wap'],df_sub2_480['wap2'],df_sub3_480['wap3'],df_sub4_480['wap4'],df_sub5_480['mid_price']],axis=1)\n",
    "            df_sub_480 = df_sub_480.rename(columns={'wap': 'rv_480', 'wap2_480': 'rv2', 'wap3_480': 'rv3', 'wap4':'rv4_480','mid_price':'rv5_480'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_480'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_480'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_480'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_480'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_480'])\n",
    "            df_sub_480 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "\n",
    "        \n",
    "        list_rv3.append(df_sub_480)\n",
    "\n",
    "        # Calculate other financial metrics from book \n",
    "        df_sub_book_feats = book_stock.groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={0:'embedding'})\n",
    "        df_sub_book_feats[['wap_imbalance','price_spread','bid_spread','ask_spread','total_vol','vol_imbalance']] = pd.DataFrame(df_sub_book_feats.embedding.tolist(), index=df_sub_book_feats.index)\n",
    "        df_sub_book_feats['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats['time_id']] \n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "\n",
    "        list_fin.append(df_sub_book_feats)\n",
    "            \n",
    "        if isEmpty300 == False:\n",
    "            df_sub_book_feats_300 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={0:'embedding'})\n",
    "            df_sub_book_feats_300[['wap_imbalance5','price_spread5','bid_spread5','ask_spread5','total_vol5','vol_imbalance5']] = pd.DataFrame(df_sub_book_feats_300.embedding.tolist(), index=df_sub_book_feats_300.index)\n",
    "            df_sub_book_feats_300['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats_300['time_id']] \n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['wap_imbalance5']) \n",
    "            temp2 = pd.DataFrame([0],columns=['price_spread5'])\n",
    "            temp3 = pd.DataFrame([0],columns=['bid_spread5'])\n",
    "            temp4 = pd.DataFrame([0],columns=['ask_spread5'])\n",
    "            temp5 = pd.DataFrame([0],columns=['total_vol5'])\n",
    "            temp6 = pd.DataFrame([0],columns=['vol_imbalance5'])\n",
    "            df_sub_book_feats_300 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1) \n",
    "            \n",
    "        list_fin2.append(df_sub_book_feats_300)\n",
    "        \n",
    "        print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Create features dataframe\n",
    "    df_submission = pd.concat(list_rv)\n",
    "    df_submission2 = pd.concat(list_rv2)\n",
    "    df_submission3 = pd.concat(list_rv3)\n",
    "    df_fin_concat = pd.concat(list_fin)\n",
    "    df_fin2_concat = pd.concat(list_fin2)\n",
    "\n",
    "    df_book_features = df_submission.merge(df_submission2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_submission3, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin2_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    \n",
    "    return df_book_features\n",
    "\n",
    "def computeFeatures_newTest_Laurent_wTrades(machine, dataset, all_stocks_ids, datapath):\n",
    "    \n",
    "    list_rv, list_rv2, list_rv3 = [], [], []\n",
    "    list_ent, list_fin, list_fin2 = [], [], []\n",
    "    list_others, list_others2, list_others3 = [], [], []\n",
    "\n",
    "    for stock_id in range(127):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if machine == 'local':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id(stock_id,datapath,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        elif machine == 'kaggle':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id_kaggle(stock_id,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Useful\n",
    "        all_time_ids_byStock = book_stock['time_id'].unique() \n",
    "\n",
    "        # Calculate wap for the entire book\n",
    "        book_stock['wap'] = calc_wap(book_stock)\n",
    "        book_stock['wap2'] = calc_wap2(book_stock)\n",
    "        book_stock['wap3'] = calc_wap3(book_stock)\n",
    "        book_stock['wap4'] = calc_wap2(book_stock)\n",
    "        book_stock['mid_price'] = calc_wap3(book_stock)\n",
    "\n",
    "        # Calculate past realized volatility per time_id\n",
    "        df_sub = book_stock.groupby('time_id')['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub2 = book_stock.groupby('time_id')['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub3 = book_stock.groupby('time_id')['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub4 = book_stock.groupby('time_id')['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub5 = book_stock.groupby('time_id')['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        \n",
    "        df_sub['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub['time_id']]\n",
    "        df_sub = df_sub.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        df_sub = pd.concat([df_sub,df_sub2['wap2'],df_sub3['wap3'], df_sub4['wap4'], df_sub5['mid_price']],axis=1)\n",
    "        df_sub = df_sub.rename(columns={'wap': 'rv', 'wap2': 'rv2', 'wap3': 'rv3', 'wap4':'rv4','mid_price':'rv5'})\n",
    "        \n",
    "        list_rv.append(df_sub)\n",
    "        \n",
    "        # Query segments\n",
    "        bucketQuery480 = book_stock.query(f'seconds_in_bucket >= 480')\n",
    "        isEmpty480 = bucketQuery480.empty\n",
    "        \n",
    "        bucketQuery300 = book_stock.query(f'seconds_in_bucket >= 300')\n",
    "        isEmpty300 = bucketQuery300.empty\n",
    "        \n",
    "        times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "        times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "        times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        # Calculate past realized volatility per time_id and query subset\n",
    "        if isEmpty300 == False:\n",
    "            df_sub_300 = bucketQuery300.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_300 = bucketQuery300.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_300 = bucketQuery300.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_300 = bucketQuery300.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_300 = bucketQuery300.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_300 = pd.concat([times_pd,df_sub_300['wap'],df_sub2_300['wap2'],df_sub3_300['wap3'],df_sub4_300['wap4'],df_sub5_300['mid_price']],axis=1)\n",
    "            df_sub_300 = df_sub_300.rename(columns={'wap': 'rv_300', 'wap2_300': 'rv2', 'wap3_300': 'rv3', 'wap4':'rv4_300','mid_price':'rv5_300'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_300'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_300'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_300'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_300'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_300'])\n",
    "            df_sub_300 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "            \n",
    "        list_rv2.append(df_sub_300)\n",
    "        \n",
    "        # Calculate realized volatility last 2 min\n",
    "        if isEmpty480 == False:\n",
    "            df_sub_480 = bucketQuery480.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_480 = bucketQuery480.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_480 = bucketQuery480.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_480 = bucketQuery480.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_480 = bucketQuery480.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_480 = pd.concat([times_pd,df_sub_480['wap'],df_sub2_480['wap2'],df_sub3_480['wap3'],df_sub4_480['wap4'],df_sub5_480['mid_price']],axis=1)\n",
    "            df_sub_480 = df_sub_480.rename(columns={'wap': 'rv_480', 'wap2_480': 'rv2', 'wap3_480': 'rv3', 'wap4':'rv4_480','mid_price':'rv5_480'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_480'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_480'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_480'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_480'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_480'])\n",
    "            df_sub_480 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "\n",
    "        \n",
    "        list_rv3.append(df_sub_480)\n",
    "\n",
    "        # Calculate other financial metrics from book \n",
    "        df_sub_book_feats = book_stock.groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={0:'embedding'})\n",
    "        df_sub_book_feats[['wap_imbalance','price_spread','bid_spread','ask_spread','total_vol','vol_imbalance']] = pd.DataFrame(df_sub_book_feats.embedding.tolist(), index=df_sub_book_feats.index)\n",
    "        df_sub_book_feats['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats['time_id']] \n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "\n",
    "        list_fin.append(df_sub_book_feats)\n",
    "            \n",
    "        if isEmpty300 == False:\n",
    "            df_sub_book_feats_300 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={0:'embedding'})\n",
    "            df_sub_book_feats_300[['wap_imbalance5','price_spread5','bid_spread5','ask_spread5','total_vol5','vol_imbalance5']] = pd.DataFrame(df_sub_book_feats_300.embedding.tolist(), index=df_sub_book_feats_300.index)\n",
    "            df_sub_book_feats_300['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats_300['time_id']] \n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['wap_imbalance5']) \n",
    "            temp2 = pd.DataFrame([0],columns=['price_spread5'])\n",
    "            temp3 = pd.DataFrame([0],columns=['bid_spread5'])\n",
    "            temp4 = pd.DataFrame([0],columns=['ask_spread5'])\n",
    "            temp5 = pd.DataFrame([0],columns=['total_vol5'])\n",
    "            temp6 = pd.DataFrame([0],columns=['vol_imbalance5'])\n",
    "            df_sub_book_feats_300 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1) \n",
    "            \n",
    "        list_fin2.append(df_sub_book_feats_300)\n",
    "        \n",
    "        print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Create features dataframe\n",
    "    df_submission = pd.concat(list_rv)\n",
    "    df_submission2 = pd.concat(list_rv2)\n",
    "    df_submission3 = pd.concat(list_rv3)\n",
    "    df_fin_concat = pd.concat(list_fin)\n",
    "    df_fin2_concat = pd.concat(list_fin2)\n",
    "\n",
    "    df_book_features = df_submission.merge(df_submission2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_submission3, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin2_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    \n",
    "    return df_book_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
