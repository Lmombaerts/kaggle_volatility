{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support file (functions)\n",
    "@LaurentMombaerts 15/07/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting support_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile support_file.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import time \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from information_measures import *\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def log_return(list_stock_prices): # Stock prices are estimated through wap values\n",
    "    return np.log(list_stock_prices).diff() \n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "def compute_wap(book_pd):\n",
    "    wap = (book_pd['bid_price1'] * book_pd['ask_size1'] + book_pd['ask_price1'] * book_pd['bid_size1']) / (book_pd['bid_size1']+ book_pd['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def realized_volatility_from_book_pd(book_stock_time):\n",
    "    \n",
    "    wap = compute_wap(book_stock_time)\n",
    "    returns = log_return(wap)\n",
    "    volatility = realized_volatility(returns)\n",
    "    \n",
    "    return volatility\n",
    "\n",
    "def realized_volatility_per_time_id(file_path, prediction_column_name):\n",
    "    df_book_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Estimate stock price per time point\n",
    "    df_book_data['wap'] = compute_wap(df_book_data)\n",
    "    \n",
    "    # Compute log return from wap values per time_id\n",
    "    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
    "    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n",
    "    \n",
    "    # Compute the square root of the sum of log return squared to get realized volatility\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n",
    "    \n",
    "    # Formatting\n",
    "    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    \n",
    "    return df_realized_vol_per_stock[['row_id',prediction_column_name]]\n",
    "\n",
    "def past_realized_volatility_per_stock(list_file,prediction_column_name):\n",
    "    df_past_realized = pd.DataFrame()\n",
    "    for file in list_file:\n",
    "        df_past_realized = pd.concat([df_past_realized,\n",
    "                                     realized_volatility_per_time_id(file,prediction_column_name)])\n",
    "    return df_past_realized\n",
    "\n",
    "def stupidForestPrediction(book_path_train,prediction_column_name,train_targets_pd,book_path_test):\n",
    "    naive_predictions_train = past_realized_volatility_per_stock(list_file=book_path_train,prediction_column_name=prediction_column_name)\n",
    "    df_joined_train = train_targets_pd.merge(naive_predictions_train[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "    \n",
    "    X = np.array(df_joined_train['pred']).reshape(-1,1)\n",
    "    y = np.array(df_joined_train['target']).reshape(-1,)\n",
    "\n",
    "    regr = RandomForestRegressor(random_state=0)\n",
    "    regr.fit(X, y)\n",
    "    \n",
    "    naive_predictions_test = past_realized_volatility_per_stock(list_file=book_path_test,prediction_column_name='target')\n",
    "    yhat = regr.predict(np.array(naive_predictions_test['target']).reshape(-1,1))\n",
    "    \n",
    "    updated_predictions = naive_predictions_test.copy()\n",
    "    updated_predictions['target'] = yhat\n",
    "    \n",
    "    return updated_predictions\n",
    "\n",
    "def entropy_from_book(book_stock_time,last_min):\n",
    "    \n",
    "    if last_min < 10:\n",
    "        book_stock_time = book_stock_time[book_stock_time['seconds_in_bucket'] >= (600-last_min*60)]\n",
    "        if book_stock_time.empty == True or book_stock_time.shape[0] < 3:\n",
    "            return 0\n",
    "        \n",
    "    wap = compute_wap(book_stock_time)\n",
    "    t_init = book_stock_time['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, wap, kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def entropy_from_wap(wap,seconds,last_seconds):\n",
    "    \n",
    "    if last_seconds < 600:\n",
    "        idx = np.where(seconds >= last_seconds)[0]\n",
    "        if len(idx) < 3:\n",
    "            return 0\n",
    "        else:\n",
    "            wap = wap[idx]\n",
    "            seconds = seconds[idx]\n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    t_new = np.arange(np.min(seconds),np.max(seconds))\n",
    "    nearest = interp1d(seconds, wap, kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    # sampleEntropy = ApEn_new(resampled_wap,3,0.001)\n",
    "    \n",
    "    return sampleEntropy\n",
    "    \n",
    "def ApEn_new(U, m, r):\n",
    "    U = np.array(U)\n",
    "    N = U.shape[0]\n",
    "            \n",
    "    def _phi(m):\n",
    "        z = N - m + 1.0\n",
    "        x = np.array([U[i:i+m] for i in range(int(z))])\n",
    "        X = np.repeat(x[:, np.newaxis], 1, axis=2)\n",
    "        C = np.sum(np.absolute(x - X).max(axis=2) <= r, axis=0) / z\n",
    "        return np.log(C).sum() / z\n",
    "    \n",
    "    return abs(_phi(m + 1) - _phi(m))\n",
    "\n",
    "def linearFit(book_stock_time, last_min):\n",
    "    \n",
    "    if last_min < 10:\n",
    "        book_stock_time = book_stock_time[book_stock_time['seconds_in_bucket'] >= (600-last_min*60)]\n",
    "        if book_stock_time.empty == True or book_stock_time.shape[0] < 2:\n",
    "            return 0\n",
    "        \n",
    "    wap = np.array(compute_wap(book_stock_time))\n",
    "    t_init = book_stock_time['seconds_in_bucket']\n",
    "    \n",
    "    return (wap[-1] - wap[0])/(np.max(t_init) - np.min(t_init))\n",
    "\n",
    "def wapStat(book_stock_time, last_min):\n",
    "    \n",
    "    if last_min < 10:\n",
    "        book_stock_time = book_stock_time[book_stock_time['seconds_in_bucket'] >= (600-last_min*60)]\n",
    "        if book_stock_time.empty == True or book_stock_time.shape[0] < 2:\n",
    "            return 0\n",
    "        \n",
    "    wap = compute_wap(book_stock_time)\n",
    "    t_init = book_stock_time['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, wap, kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    return np.std(resampled_wap)\n",
    "\n",
    "\n",
    "def entropy_Prediction(book_path_train,prediction_column_name,train_targets_pd,book_path_test,all_stocks_ids,test_file):\n",
    "    \n",
    "    # Compute features\n",
    "    book_features_encoded_test = computeFeatures_1(book_path_test,'test',test_file,all_stocks_ids) \n",
    "    \n",
    "    book_features_encoded_train = computeFeatures_1(book_path_train,'train',train_targets_pd,all_stocks_ids)\n",
    "    \n",
    "    X = book_features_encoded_train.drop(['row_id','target','stock_id'],axis=1)\n",
    "    y = book_features_encoded_train['target']\n",
    "    \n",
    "    # Modeling\n",
    "    catboost_default = CatBoostRegressor(verbose=0)\n",
    "    catboost_default.fit(X,y)\n",
    "    \n",
    "    # Predict\n",
    "    X_test = book_features_encoded_test.drop(['row_id','stock_id'],axis=1)\n",
    "    yhat = catboost_default.predict(X_test)\n",
    "    \n",
    "    # Formatting\n",
    "    yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "    predictions = pd.concat([test_file,yhat_pd],axis=1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def computeFeatures_1(book_path,prediction_column_name,train_targets_pd,all_stocks_ids):\n",
    "    \n",
    "    book_all_features = pd.DataFrame()\n",
    "    encoder = np.eye(len(all_stocks_ids))\n",
    "\n",
    "    stocks_id_list, row_id_list = [], []\n",
    "    volatility_list, entropy2_list = [], []\n",
    "    linearFit_list, linearFit5_list, linearFit2_list = [], [], []\n",
    "    wap_std_list, wap_std5_list, wap_std2_list = [], [], []\n",
    "\n",
    "    for file in book_path:\n",
    "        start = time.time()\n",
    "\n",
    "        book_stock = pd.read_parquet(file)\n",
    "        stock_id = file.split('=')[1]\n",
    "        print('stock id computing = ' + str(stock_id))\n",
    "        stock_time_ids = book_stock['time_id'].unique()\n",
    "        for time_id in stock_time_ids:     \n",
    "\n",
    "            # Access book data at this time + stock\n",
    "            book_stock_time = book_stock[book_stock['time_id'] == time_id]\n",
    "\n",
    "            # Create feature matrix\n",
    "            stocks_id_list.append(stock_id)\n",
    "            row_id_list.append(str(f'{stock_id}-{time_id}'))\n",
    "            volatility_list.append(realized_volatility_from_book_pd(book_stock_time=book_stock_time))\n",
    "            entropy2_list.append(entropy_from_book(book_stock_time=book_stock_time,last_min=2))\n",
    "            linearFit_list.append(linearFit(book_stock_time=book_stock_time,last_min=10))\n",
    "            linearFit5_list.append(linearFit(book_stock_time=book_stock_time,last_min=5))\n",
    "            linearFit2_list.append(linearFit(book_stock_time=book_stock_time,last_min=2))\n",
    "            wap_std_list.append(wapStat(book_stock_time=book_stock_time,last_min=10))\n",
    "            wap_std5_list.append(wapStat(book_stock_time=book_stock_time,last_min=5))\n",
    "            wap_std2_list.append(wapStat(book_stock_time=book_stock_time,last_min=2))\n",
    "\n",
    "        print('Computing one stock entropy took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Merge targets\n",
    "    stocks_id_pd = pd.DataFrame(stocks_id_list,columns=['stock_id'])\n",
    "    row_id_pd = pd.DataFrame(row_id_list,columns=['row_id'])\n",
    "    volatility_pd = pd.DataFrame(volatility_list,columns=['volatility'])\n",
    "    entropy2_pd = pd.DataFrame(entropy2_list,columns=['entropy2'])\n",
    "    linearFit_pd = pd.DataFrame(linearFit_list,columns=['linearFit_coef'])\n",
    "    linearFit5_pd = pd.DataFrame(linearFit5_list,columns=['linearFit_coef5'])\n",
    "    linearFit2_pd = pd.DataFrame(linearFit2_list,columns=['linearFit_coef2'])\n",
    "    wap_std_pd = pd.DataFrame(wap_std_list,columns=['wap_std'])\n",
    "    wap_std5_pd = pd.DataFrame(wap_std5_list,columns=['wap_std5'])\n",
    "    wap_std2_pd = pd.DataFrame(wap_std2_list,columns=['wap_std2'])\n",
    "\n",
    "    book_all_features = pd.concat([stocks_id_pd,row_id_pd,volatility_pd,entropy2_pd,linearFit_pd,linearFit5_pd,linearFit2_pd,\n",
    "                                  wap_std_pd,wap_std5_pd,wap_std2_pd],axis=1)\n",
    "\n",
    "    # This line makes sure the predictions are aligned with the row_id in the submission file\n",
    "    book_all_features = train_targets_pd.merge(book_all_features, on = ['row_id'])\n",
    "\n",
    "    # Add encoded stock\n",
    "    encoded = list()\n",
    "\n",
    "    for i in range(book_all_features.shape[0]):\n",
    "        stock_id = book_all_features['stock_id'][i]\n",
    "        encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]\n",
    "        encoded.append(encoded_stock)\n",
    "\n",
    "    encoded_pd = pd.DataFrame(np.array(encoded).reshape(book_all_features.shape[0],np.array(all_stocks_ids).shape[0]))\n",
    "    book_all_features_encoded = pd.concat([book_all_features, encoded_pd],axis=1)\n",
    "    \n",
    "    return book_all_features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
