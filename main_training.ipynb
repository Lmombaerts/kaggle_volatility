{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for Kaggle - Optiver Realized Volatility Prediction\n",
    "@LaurentMombaerts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lib Import / Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Maths\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Paths tricks\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Support code\n",
    "from support_file import *\n",
    "from information_measures import *\n",
    "\n",
    "datapath = os.path.join(str(Path.home()), 'ownCloud', 'Data', 'Kaggle', 'optiver-realized-volatility-prediction')\n",
    "\n",
    "# Load dataset\n",
    "train = pd.read_csv(os.path.join(datapath,'train.csv')) \n",
    "all_stocks_ids = train['stock_id'].unique()\n",
    "all_time_ids = train['time_id'].unique()\n",
    "\n",
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "train = train[['row_id','target']]\n",
    "\n",
    "# Load test ids\n",
    "test = pd.read_csv(os.path.join(datapath,'test.csv'))\n",
    "test = test.drop(['stock_id','time_id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Prediction function (chose here which prediction strategy to use)\n",
    "def prediction_function(pred,book_path_train,trade_path_train,targets,book_path_test,trade_path_test,all_stocks_ids,test_file):\n",
    "    \n",
    "    if pred == 'naive':\n",
    "        # Naive prediction (persistence model)\n",
    "        prediction = past_realized_volatility_per_stock(list_file=book_path_train,prediction_column_name='pred')\n",
    "        \n",
    "        # Merge and evaluate results\n",
    "        prediction = train.merge(prediction[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "        print(prediction.head(5))\n",
    "\n",
    "        # Estimate performances\n",
    "        R2 = round(r2_score(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "        RMSPE = round(rmspe(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "\n",
    "        print('--')\n",
    "        print(f'Performance of prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n",
    "        \n",
    "        prediction = prediction.drop(columns=['target'])\n",
    "        prediction = prediction.rename(columns={'pred': 'target'})\n",
    "\n",
    "    if pred == 'stupid_RF':\n",
    "        # Stupid nonlinear regression between persistence and next volatility (random forest)\n",
    "        prediction = stupidForestPrediction(book_path_train=book_path_train,\n",
    "                                            prediction_column_name='pred',\n",
    "                                            train_targets_pd=targets,\n",
    "                                            book_path_test=book_path_test)\n",
    "        \n",
    "    if pred == 'entropy':\n",
    "        prediction = entropy_Prediction(book_path_train=book_path_train,\n",
    "                                            prediction_column_name='pred',\n",
    "                                            train_targets_pd=targets,\n",
    "                                            book_path_test=book_path_test,\n",
    "                                            all_stocks_ids=all_stocks_ids,\n",
    "                                            test_file=test_file)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a prediction code\n",
    "\n",
    "# Glob book file train (contains all paths for each file in this folder)\n",
    "list_order_book_file_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "list_order_book_file_test = glob.glob(os.path.join(datapath,'book_test.parquet','*'))\n",
    "list_trade_file_train = glob.glob(os.path.join(datapath,'trade_train.parquet','*')) \n",
    "list_trade_file_test = glob.glob(os.path.join(datapath,'trade_test.parquet','*'))\n",
    "\n",
    "# Given variables\n",
    "pred = 'entropy_based'\n",
    "book_path_train = list_order_book_file_train\n",
    "trade_path_train = list_trade_file_train\n",
    "targets = train\n",
    "book_path_test = list_order_book_file_test\n",
    "trade_path_test = list_trade_file_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laurent\\Documents\\work\\git\\kaggle_volatility\\information_measures.py:798: RuntimeWarning: Zero vectors are within tolerance for emb_dim and emb_dim + 1. Consider raising the tolerance parameter to avoid NaN result.\n",
      "  RuntimeWarning\n",
      "C:\\Users\\laurent\\Documents\\work\\git\\kaggle_volatility\\information_measures.py:798: RuntimeWarning: Zero vectors are within tolerance for emb_dim and emb_dim + 1. Consider raising the tolerance parameter to avoid NaN result.\n",
      "  RuntimeWarning\n"
     ]
    }
   ],
   "source": [
    "# Computational time optimized with groupby numba\n",
    "\n",
    "def calc_wap(df):\n",
    "    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def calc_wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def calc_wap3(df):\n",
    "    return (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def calc_rv_from_wap_numba(values, index):\n",
    "    log_return = np.diff(np.log(values))\n",
    "    realized_vol = np.sqrt(np.sum(np.square(log_return[1:])))\n",
    "    return realized_vol\n",
    "\n",
    "def load_book_data_by_id(stock_id,datapath,train_test):\n",
    "    file_to_read = os.path.join(datapath,'book_' + train_test + str('.parquet'),'stock_id=' + str(stock_id))\n",
    "    df = pd.read_parquet(file_to_read)\n",
    "    return df\n",
    "\n",
    "def entropy_from_df(df):\n",
    "    \n",
    "    if df.shape[0] < 3:\n",
    "        return 0\n",
    "        \n",
    "    t_init = df['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, df['wap'], kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def entropy_from_df2(df):\n",
    "    \n",
    "    if df.shape[0] < 3:\n",
    "        return 0\n",
    "        \n",
    "    t_init = df['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, df['wap2'], kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    # sampleEntropy = nolds.sampen(resampled_wap)\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def entropy_from_df3(df):\n",
    "    \n",
    "    if df.shape[0] < 3:\n",
    "        return 0\n",
    "        \n",
    "    t_init = df['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, df['wap3'], kind='nearest')\n",
    "    resampled_wap = nearest(t_new)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    sampleEntropy = sampen(resampled_wap)\n",
    "    \n",
    "    return sampleEntropy\n",
    "\n",
    "def financial_metrics(df):\n",
    "    \n",
    "    wap_imbalance = np.mean(df['wap'] - df['wap2'])\n",
    "    price_spread = np.mean((df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2))\n",
    "    bid_spread = np.mean(df['bid_price1'] - df['bid_price2'])\n",
    "    ask_spread = np.mean(df['ask_price1'] - df['ask_price2'])\n",
    "    total_volume = np.mean((df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2']))\n",
    "    volume_imbalance = np.mean(abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2'])))\n",
    "    \n",
    "    return [wap_imbalance,price_spread,bid_spread,ask_spread,total_volume,volume_imbalance]\n",
    "\n",
    "def other_metrics(df):\n",
    "    \n",
    "    linearFit = (df['wap'].iloc[-1] - df['wap'].iloc[0]) / ((np.max(df['seconds_in_bucket']) - np.min(df['seconds_in_bucket']))) \n",
    "    linearFit2 = (df['wap2'].iloc[-1] - df['wap2'].iloc[0]) / ((np.max(df['seconds_in_bucket']) - np.min(df['seconds_in_bucket']))) \n",
    "    linearFit3 = (df['wap3'].iloc[-1] - df['wap3'].iloc[0]) / ((np.max(df['seconds_in_bucket']) - np.min(df['seconds_in_bucket']))) \n",
    "    \n",
    "    # Resampling\n",
    "    t_init = df['seconds_in_bucket']\n",
    "    t_new = np.arange(np.min(t_init),np.max(t_init)) \n",
    "    \n",
    "    # Closest neighbour interpolation (no changes in wap between lines)\n",
    "    nearest = interp1d(t_init, df['wap'], kind='nearest')\n",
    "    nearest2 = interp1d(t_init, df['wap2'], kind='nearest')\n",
    "    nearest3 = interp1d(t_init, df['wap3'], kind='nearest')\n",
    "    \n",
    "    std_1 = np.std(nearest(t_new))\n",
    "    std_2 = np.std(nearest2(t_new))\n",
    "    std_3 = np.std(nearest3(t_new))\n",
    "    \n",
    "    return [linearFit, linearFit2, linearFit3, std_1, std_2, std_3]\n",
    "\n",
    "\n",
    "book_all_features = pd.DataFrame()\n",
    "encoder = np.eye(len(all_stocks_ids))\n",
    "\n",
    "list_rv, list_rv2, list_rv3 = [], [], []\n",
    "list_ent, list_fin, list_fin2 = [], [], []\n",
    "list_others, list_others2, list_others3 = [], [], []\n",
    "\n",
    "for stock_id in range(127):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        book_stock = load_book_data_by_id(stock_id,datapath,'train')\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Calculate wap for the book\n",
    "    book_stock['wap'] = calc_wap(book_stock)\n",
    "    book_stock['wap2'] = calc_wap2(book_stock)\n",
    "    book_stock['wap3'] = calc_wap3(book_stock)\n",
    "    \n",
    "    # Calculate realized volatility\n",
    "    df_sub = book_stock.groupby('time_id')['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "    df_sub2 = book_stock.groupby('time_id')['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index().drop(['time_id'],axis=1)\n",
    "    df_sub3 = book_stock.groupby('time_id')['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "    df_sub['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub['time_id']]\n",
    "    df_sub = pd.concat([df_sub,df_sub2['wap2'],df_sub3['wap3']],axis=1)\n",
    "    df_sub = df_sub.rename(columns={'time_id':'row_id','wap': 'rv', 'wap2': 'rv2', 'wap3': 'rv3'})\n",
    "    \n",
    "    # Calculate realized volatility last 5 min\n",
    "    df_sub_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "    df_sub2_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "    df_sub3_5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "    df_sub_5['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_5['time_id']]\n",
    "    df_sub_5 = pd.concat([df_sub_5,df_sub2_5['wap2'],df_sub3_5['wap3']],axis=1)\n",
    "    df_sub_5 = df_sub_5.rename(columns={'time_id':'row_id','wap': 'rv_5', 'wap2': 'rv2_5', 'wap3': 'rv3_5'})\n",
    "    \n",
    "    # Calculate realized volatility last 2 min\n",
    "    df_sub_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "    df_sub2_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "    df_sub3_2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()    \n",
    "    df_sub_2['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_2['time_id']] \n",
    "    df_sub_2 = pd.concat([df_sub_2,df_sub2_2['wap2'],df_sub3_2['wap3']],axis=1)\n",
    "    df_sub_2 = df_sub_2.rename(columns={'time_id':'row_id','wap': 'rv_2', 'wap2': 'rv2_2', 'wap3': 'rv3_2'})\n",
    "    \n",
    "    list_rv.append(df_sub)\n",
    "    list_rv2.append(df_sub_5)\n",
    "    list_rv3.append(df_sub_2)\n",
    "    \n",
    "    # Calculate other financial metrics from book \n",
    "    df_sub_book_feats = book_stock.groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "    df_sub_book_feats = df_sub_book_feats.rename(columns={0:'embedding'})\n",
    "    df_sub_book_feats[['wap_imbalance','price_spread','bid_spread','ask_spread','total_vol','vol_imbalance']] = pd.DataFrame(df_sub_book_feats.embedding.tolist(), index=df_try.index)\n",
    "    df_sub_book_feats['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats['time_id']] \n",
    "    df_sub_book_feats = df_sub_book_feats.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "    \n",
    "    df_sub_book_feats5 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "    df_sub_book_feats5 = df_sub_book_feats5.rename(columns={0:'embedding'})\n",
    "    df_sub_book_feats5[['wap_imbalance5','price_spread5','bid_spread5','ask_spread5','total_vol5','vol_imbalance5']] = pd.DataFrame(df_sub_book_feats5.embedding.tolist(), index=df_try.index)\n",
    "    df_sub_book_feats5['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats5['time_id']] \n",
    "    df_sub_book_feats5 = df_sub_book_feats5.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "    \n",
    "    list_fin.append(df_sub_book_feats)\n",
    "    list_fin2.append(df_sub_book_feats5)\n",
    "    \n",
    "    # Compute entropy \n",
    "    df_ent = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(entropy_from_df).to_frame().reset_index().fillna(0)\n",
    "    df_ent2 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(entropy_from_df2).to_frame().reset_index().fillna(0)\n",
    "    df_ent3 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(entropy_from_df3).to_frame().reset_index().fillna(0)\n",
    "    df_ent['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_ent['time_id']]\n",
    "    df_ent = df_ent.rename(columns={'time_id':'row_id',0:'entropy'})\n",
    "    df_ent2 = df_ent2.rename(columns={0:'entropy2'}).drop(['time_id'],axis=1)\n",
    "    df_ent3 = df_ent3.rename(columns={0:'entropy3'}).drop(['time_id'],axis=1)\n",
    "    df_ent = pd.concat([df_ent,df_ent2,df_ent3],axis=1)\n",
    "    list_ent.append(df_ent)\n",
    "    \n",
    "    # Compute other metrics\n",
    "    df_others = book_stock.groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "    df_others = df_others.rename(columns={0:'embedding'})\n",
    "    df_others[['linearFit1_1','linearFit1_2','linearFit1_3','wap_std1_1','wap_std1_2','wap_std1_3']] = pd.DataFrame(df_others.embedding.tolist(), index=df_try.index)\n",
    "    df_others['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others['time_id']] \n",
    "    df_others = df_others.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "    list_others.append(df_others)\n",
    "    \n",
    "    df_others2 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "    df_others2 = df_others2.rename(columns={0:'embedding'})\n",
    "    df_others2[['linearFit2_1','linearFit2_2','linearFit2_3','wap_std2_1','wap_std2_2','wap_std2_3']] = pd.DataFrame(df_others2.embedding.tolist(), index=df_try.index)\n",
    "    df_others2['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others2['time_id']] \n",
    "    df_others2 = df_others2.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "    list_others2.append(df_others2)\n",
    "    \n",
    "    df_others3 = book_stock.query(f'seconds_in_bucket >= 480').groupby(['time_id']).apply(other_metrics).to_frame().reset_index().fillna(0)\n",
    "    df_others3 = df_others3.rename(columns={0:'embedding'})\n",
    "    df_others3[['linearFit3_1','linearFit3_2','linearFit3_3','wap_std3_1','wap_std3_2','wap_std3_3']] = pd.DataFrame(df_others3.embedding.tolist(), index=df_try.index)\n",
    "    df_others3['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_others3['time_id']] \n",
    "    df_others3 = df_others3.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "    list_others3.append(df_others3)\n",
    "    \n",
    "    print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "    \n",
    "# Create features dataframe\n",
    "df_submission = pd.concat(list_rv)\n",
    "df_submission2 = pd.concat(list_rv2)\n",
    "df_submission3 = pd.concat(list_rv3)\n",
    "df_ent_concat = pd.concat(list_ent)\n",
    "df_fin_concat = pd.concat(list_fin)\n",
    "df_fin2_concat = pd.concat(list_fin2)\n",
    "df_others = pd.concat(list_others)\n",
    "df_others2 = pd.concat(list_others2)\n",
    "df_others3 = pd.concat(list_others3)\n",
    "\n",
    "df_book_features = df_submission.merge(df_submission2, on = ['row_id'], how='left').fillna(0)\n",
    "df_book_features = df_book_features.merge(df_submission3, on = ['row_id'], how='left').fillna(0)\n",
    "df_book_features = df_book_features.merge(df_ent_concat, on = ['row_id'], how='left').fillna(0)\n",
    "df_book_features = df_book_features.merge(df_fin_concat, on = ['row_id'], how='left').fillna(0)\n",
    "df_book_features = df_book_features.merge(df_fin2_concat, on = ['row_id'], how='left').fillna(0)\n",
    "df_book_features = df_book_features.merge(df_others, on = ['row_id'], how='left').fillna(0)\n",
    "df_book_features = df_book_features.merge(df_others2, on = ['row_id'], how='left').fillna(0)\n",
    "df_book_features = df_book_features.merge(df_others3, on = ['row_id'], how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_features.to_csv('book_features_tot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entropy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-2ef9002d1e66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mseconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrouped_seconds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#entropies.append(entropy_from_wap(wap,seconds,120))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mentropies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Computing one stock took'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seconds for stock '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'entropy' is not defined"
     ]
    }
   ],
   "source": [
    "### print(book_stock['time_id'].nunique())\n",
    "start = time.time()\n",
    "grouped_wap = list(book_stock.groupby('time_id')['wap'])\n",
    "grouped_seconds = list(book_stock.groupby('time_id')['seconds_in_bucket'])\n",
    "entropies = list()\n",
    "for time_id in range(book_stock['time_id'].nunique()):\n",
    "    wap = np.array(grouped_wap[time_id][1])\n",
    "    seconds = np.array(grouped_seconds[time_id][1])\n",
    "    #entropies.append(entropy_from_wap(wap,seconds,120))\n",
    "    entropies.append(entropy(pd.Series(wap).value_counts()))\n",
    "    \n",
    "print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "#np.array(list(book_stock.groupby('time_id')['seconds_in_bucket'])[time_id_list][1])\n",
    "\n",
    "print(seconds)\n",
    "idx = np.where(seconds > 590)[0]\n",
    "print(len(idx))\n",
    "#df_entropies = pd.concat(entropies)\n",
    "#df_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027788768059913704"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "entropy(pd.Series(wap).value_counts())\n",
    "#pd.Series(wap).value_counts().to_frame().to_csv('test_shannon.csv')\n",
    "\n",
    "def ApEn_new(U, m, r):\n",
    "    U = np.array(U)\n",
    "    N = U.shape[0]\n",
    "            \n",
    "    def _phi(m):\n",
    "        z = N - m + 1.0\n",
    "        x = np.array([U[i:i+m] for i in range(int(z))])\n",
    "        X = np.repeat(x[:, np.newaxis], 1, axis=2)\n",
    "        C = np.sum(np.absolute(x - X).max(axis=2) <= r, axis=0) / z\n",
    "        return np.log(C).sum() / z\n",
    "    \n",
    "    return abs(_phi(m + 1) - _phi(m))\n",
    "\n",
    "wap\n",
    "ApEn_new(wap,3,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-5</td>\n",
       "      <td>0.004499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-11</td>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-16</td>\n",
       "      <td>0.002330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-31</td>\n",
       "      <td>0.002574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-62</td>\n",
       "      <td>0.001880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>0-32751</td>\n",
       "      <td>0.002539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>0-32753</td>\n",
       "      <td>0.002173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>0-32758</td>\n",
       "      <td>0.002913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828</th>\n",
       "      <td>0-32763</td>\n",
       "      <td>0.003046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>0-32767</td>\n",
       "      <td>0.001893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3830 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id    target\n",
       "0         0-5  0.004499\n",
       "1        0-11  0.001204\n",
       "2        0-16  0.002330\n",
       "3        0-31  0.002574\n",
       "4        0-62  0.001880\n",
       "...       ...       ...\n",
       "3825  0-32751  0.002539\n",
       "3826  0-32753  0.002173\n",
       "3827  0-32758  0.002913\n",
       "3828  0-32763  0.003046\n",
       "3829  0-32767  0.001893\n",
       "\n",
       "[3830 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock id computing = 0\n",
      "Computing one stock entropy took 31.55561923980713 seconds for stock  0\n",
      "stock id computing = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-dc044da39d32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mstocks_id_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mrow_id_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{stock_id}-{time_id}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mvolatility_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrealized_volatility_from_book_pd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m#entropy2_list.append(entropy_from_book(book_stock_time=book_stock_time,last_min=2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mlinearFit_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinearFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\work\\git\\kaggle_volatility\\support_file.py\u001b[0m in \u001b[0;36mrealized_volatility_from_book_pd\u001b[1;34m(book_stock_time)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mwap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_wap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mreturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mvolatility\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrealized_volatility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvolatility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\work\\git\\kaggle_volatility\\support_file.py\u001b[0m in \u001b[0;36mrealized_volatility\u001b[1;34m(series_log_return)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrealized_volatility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_log_return\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_log_return\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_wap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_pd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2241\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2242\u001b[1;33m                           initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[1;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11423\u001b[0m             \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11424\u001b[0m             \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11425\u001b[1;33m             \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11426\u001b[0m         )\n\u001b[0;32m  11427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4234\u001b[0m                 )\n\u001b[0;32m   4235\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4236\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reindex_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36m_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;31m# we want to transform an object array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mnansum\u001b[1;34m(values, axis, skipna, min_count, mask)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m    501\u001b[0m     values, mask, dtype, dtype_max, _ = _get_values(\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     \u001b[0mdtype_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype_max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36m_get_values\u001b[1;34m(values, skipna, fill_value, fill_value_typ, mask)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_ok\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# promote if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mputmask\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "book_all_features = pd.DataFrame()\n",
    "encoder = np.eye(len(all_stocks_ids))\n",
    "\n",
    "stocks_id_list, row_id_list = [], []\n",
    "volatility_list, entropy2_list = [], []\n",
    "linearFit_list, linearFit5_list, linearFit2_list = [], [], []\n",
    "wap_std_list, wap_std5_list, wap_std2_list = [], [], []\n",
    "\n",
    "for file in book_path_train:\n",
    "    start = time.time()\n",
    "    \n",
    "    book_stock = pd.read_parquet(file)\n",
    "    stock_id = file.split('=')[1]\n",
    "    print('stock id computing = ' + str(stock_id))\n",
    "    stock_time_ids = book_stock['time_id'].unique()\n",
    "    for time_id in stock_time_ids:     \n",
    "        \n",
    "        # Access book data at this time + stock\n",
    "        book_stock_time = book_stock[book_stock['time_id'] == time_id]\n",
    "\n",
    "        # Create feature matrix\n",
    "        stocks_id_list.append(stock_id)\n",
    "        row_id_list.append(str(f'{stock_id}-{time_id}'))\n",
    "        volatility_list.append(realized_volatility_from_book_pd(book_stock_time=book_stock_time))\n",
    "        entropy2_list.append(entropy_from_book(book_stock_time=book_stock_time,last_min=2))\n",
    "        linearFit_list.append(linearFit(book_stock_time=book_stock_time,last_min=10))\n",
    "        linearFit5_list.append(linearFit(book_stock_time=book_stock_time,last_min=5))\n",
    "        linearFit2_list.append(linearFit(book_stock_time=book_stock_time,last_min=2))\n",
    "        wap_std_list.append(wapStat(book_stock_time=book_stock_time,last_min=10))\n",
    "        wap_std5_list.append(wapStat(book_stock_time=book_stock_time,last_min=5))\n",
    "        wap_std2_list.append(wapStat(book_stock_time=book_stock_time,last_min=2))\n",
    "        \n",
    "    print('Computing one stock entropy took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "# Merge targets\n",
    "stocks_id_pd = pd.DataFrame(stocks_id_list,columns=['stock_id'])\n",
    "row_id_pd = pd.DataFrame(row_id_list,columns=['row_id'])\n",
    "volatility_pd = pd.DataFrame(volatility_list,columns=['volatility'])\n",
    "entropy2_pd = pd.DataFrame(entropy2_list,columns=['entropy2'])\n",
    "linearFit_pd = pd.DataFrame(linearFit_list,columns=['linearFit_coef'])\n",
    "linearFit5_pd = pd.DataFrame(linearFit5_list,columns=['linearFit_coef5'])\n",
    "linearFit2_pd = pd.DataFrame(linearFit2_list,columns=['linearFit_coef2'])\n",
    "wap_std_pd = pd.DataFrame(wap_std_list,columns=['wap_std'])\n",
    "wap_std5_pd = pd.DataFrame(wap_std5_list,columns=['wap_std5'])\n",
    "wap_std2_pd = pd.DataFrame(wap_std2_list,columns=['wap_std2'])\n",
    "\n",
    "book_all_features = pd.concat([stocks_id_pd,row_id_pd,volatility_pd,entropy2_pd,linearFit_pd,linearFit5_pd,linearFit2_pd,\n",
    "                              wap_std_pd,wap_std5_pd,wap_std2_pd],axis=1)\n",
    "\n",
    "book_all_features = train.merge(book_all_features, on = ['row_id'])\n",
    "\n",
    "# Add encoded stock\n",
    "encoded = list()\n",
    "\n",
    "for i in range(book_all_features.shape[0]):\n",
    "    stock_id = book_all_features['stock_id'][i]\n",
    "    encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]\n",
    "    encoded.append(encoded_stock)\n",
    "\n",
    "encoded_pd = pd.DataFrame(np.array(encoded).reshape(book_all_features.shape[0],np.array(all_stocks_ids).shape[0]))\n",
    "book_all_features_encoded = pd.concat([book_all_features, encoded_pd],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>volatility</th>\n",
       "      <th>entropy2</th>\n",
       "      <th>linearFit_coef</th>\n",
       "      <th>linearFit_coef5</th>\n",
       "      <th>linearFit_coef2</th>\n",
       "      <th>wap_std</th>\n",
       "      <th>wap_std5</th>\n",
       "      <th>...</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.357092</td>\n",
       "      <td>3.874554e-06</td>\n",
       "      <td>-7.296451e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.173592</td>\n",
       "      <td>6.045011e-07</td>\n",
       "      <td>2.164258e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.066508</td>\n",
       "      <td>-3.463848e-06</td>\n",
       "      <td>-8.714889e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.076961</td>\n",
       "      <td>-4.829273e-06</td>\n",
       "      <td>-4.213925e-06</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.164630</td>\n",
       "      <td>-3.157112e-09</td>\n",
       "      <td>2.151035e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126-32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.264883</td>\n",
       "      <td>-8.803732e-07</td>\n",
       "      <td>1.123249e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126-32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>0.218649</td>\n",
       "      <td>7.455736e-06</td>\n",
       "      <td>1.272477e-05</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126-32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.244481</td>\n",
       "      <td>2.550217e-06</td>\n",
       "      <td>7.278551e-06</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126-32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.423108</td>\n",
       "      <td>4.275249e-07</td>\n",
       "      <td>-5.802447e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126-32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.095729</td>\n",
       "      <td>-1.223051e-06</td>\n",
       "      <td>-1.403475e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           row_id    target stock_id  volatility  entropy2  linearFit_coef  \\\n",
       "0             0-5  0.004136        0    0.004499  0.357092    3.874554e-06   \n",
       "1            0-11  0.001445        0    0.001204  0.173592    6.045011e-07   \n",
       "2            0-16  0.002168        0    0.002369  0.066508   -3.463848e-06   \n",
       "3            0-31  0.002195        0    0.002574  0.076961   -4.829273e-06   \n",
       "4            0-62  0.001747        0    0.001894  0.164630   -3.157112e-09   \n",
       "...           ...       ...      ...         ...       ...             ...   \n",
       "428927  126-32751  0.003461      126    0.003691  0.264883   -8.803732e-07   \n",
       "428928  126-32753  0.003113      126    0.004104  0.218649    7.455736e-06   \n",
       "428929  126-32758  0.004070      126    0.003118  0.244481    2.550217e-06   \n",
       "428930  126-32763  0.003357      126    0.003661  0.423108    4.275249e-07   \n",
       "428931  126-32767  0.002090      126    0.002091  0.095729   -1.223051e-06   \n",
       "\n",
       "        linearFit_coef5  linearFit_coef2   wap_std  wap_std5  ...  102  103  \\\n",
       "0         -7.296451e-07        -0.000002  0.000698  0.000498  ...  0.0  0.0   \n",
       "1          2.164258e-07         0.000001  0.000258  0.000186  ...  0.0  0.0   \n",
       "2         -8.714889e-06        -0.000008  0.000924  0.000911  ...  0.0  0.0   \n",
       "3         -4.213925e-06        -0.000007  0.000791  0.000401  ...  0.0  0.0   \n",
       "4          2.151035e-06         0.000006  0.000265  0.000184  ...  0.0  0.0   \n",
       "...                 ...              ...       ...       ...  ...  ...  ...   \n",
       "428927     1.123249e-06         0.000005  0.000473  0.000302  ...  0.0  0.0   \n",
       "428928     1.272477e-05         0.000024  0.001142  0.000777  ...  0.0  0.0   \n",
       "428929     7.278551e-06         0.000011  0.000503  0.000443  ...  0.0  0.0   \n",
       "428930    -5.802447e-06        -0.000010  0.000466  0.000477  ...  0.0  0.0   \n",
       "428931    -1.403475e-06        -0.000008  0.000346  0.000442  ...  0.0  0.0   \n",
       "\n",
       "        104  105  106  107  108  109  110  111  \n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "428927  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "428928  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "428929  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "428930  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "428931  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[428932 rows x 123 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_all_features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistence model perf : 0.34135449018801606\n",
      "[21:08:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "New model xgb perf :  0.306084377117114\n",
      "New model lgbm perf :  0.2927659975146637\n",
      "New model catboost perf :  0.28643356812572324\n",
      "New model mean gradient boosted trees :  0.2931403264914029\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "X = book_all_features_encoded.drop(['row_id','target','stock_id'],axis=1)\n",
    "y = book_all_features_encoded['target']\n",
    "\n",
    "\n",
    "print('Persistence model perf :', rmspe(y,book_all_features_encoded['volatility']))\n",
    "\n",
    "xgboost_default = xgb.XGBRegressor(random_state=0)\n",
    "xgboost_default.fit(X,y)\n",
    "\n",
    "yhat_xgb = xgboost_default.predict(X)\n",
    "print('New model xgb perf : ', rmspe(y, yhat_xgb))\n",
    "\n",
    "lightgbm_default = LGBMRegressor()\n",
    "lightgbm_default.fit(X,y)\n",
    "yhat_light = lightgbm_default.predict(X)\n",
    "print('New model lgbm perf : ', rmspe(y, yhat_light))\n",
    "\n",
    "catboost_default = CatBoostRegressor(verbose=0)\n",
    "catboost_default.fit(X,y)\n",
    "yhat_cat = catboost_default.predict(X)\n",
    "print('New model catboost perf : ', rmspe(y, yhat_cat))\n",
    "\n",
    "print('New model mean gradient boosted trees : ', rmspe(y,(yhat_xgb + yhat_light + yhat_cat)/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main evaluation code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock id computing = 0\n",
      "Computing one stock entropy took 0.03986024856567383 seconds for stock  0\n",
      "  row_id stock_id  volatility  entropy2  linearFit_coef  linearFit_coef5  \\\n",
      "0    0-4        0    0.000294         0        0.000059                0   \n",
      "\n",
      "   linearFit_coef2   wap_std  wap_std5  wap_std2  ...  102  103  104  105  \\\n",
      "0                0  0.000118         0         0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   106  107  108  109  110  111  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1 rows x 122 columns]\n",
      "stock id computing = 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ff6ffd2686d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                  \u001b[0mtrade_path_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist_trade_file_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                  \u001b[0mall_stocks_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_stocks_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                                  test_file=test)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-155bf651e8be>\u001b[0m in \u001b[0;36mprediction_function\u001b[1;34m(pred, book_path_train, trade_path_train, targets, book_path_test, trade_path_test, all_stocks_ids, test_file)\u001b[0m\n\u001b[0;32m     37\u001b[0m                                             \u001b[0mbook_path_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_path_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                             \u001b[0mall_stocks_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_stocks_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                             test_file=test_file)\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\work\\git\\kaggle_volatility\\support_file.py\u001b[0m in \u001b[0;36mentropy_Prediction\u001b[1;34m(book_path_train, prediction_column_name, train_targets_pd, book_path_test, all_stocks_ids, test_file)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_features_encoded_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[0mbook_features_encoded_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeFeatures_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_path_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_targets_pd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_stocks_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbook_features_encoded_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'row_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'stock_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\work\\git\\kaggle_volatility\\support_file.py\u001b[0m in \u001b[0;36mcomputeFeatures_1\u001b[1;34m(book_path, prediction_column_name, train_targets_pd, all_stocks_ids)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[0mlinearFit_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinearFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mlinearFit5_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinearFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mlinearFit2_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinearFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[0mwap_std_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwapStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mwap_std5_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwapStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\work\\git\\kaggle_volatility\\support_file.py\u001b[0m in \u001b[0;36mlinearFit\u001b[1;34m(book_stock_time, last_min)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mwap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_wap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_stock_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mt_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbook_stock_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'seconds_in_bucket'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\work\\git\\kaggle_volatility\\support_file.py\u001b[0m in \u001b[0;36mcompute_wap\u001b[1;34m(book_pd)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_wap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_pd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mwap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbook_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bid_price1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbook_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ask_size1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbook_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ask_price1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbook_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bid_size1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbook_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bid_size1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mbook_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ask_size1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2876\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2877\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2878\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m         \u001b[1;31m# Do we have a slicer (on rows)?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   3536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3537\u001b[0m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3538\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3539\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_col_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36miget\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \"\"\"\n\u001b[1;32m--> 988\u001b[1;33m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mblknos\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blknos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;31m# Note: these can be altered by other BlockManager methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_blknos_and_blklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_rebuild_blknos_and_blklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[0mnew_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mnew_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_blknos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Glob book file train (contains all paths for each file in this folder)\n",
    "list_order_book_file_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "list_order_book_file_test = glob.glob(os.path.join(datapath,'book_test.parquet','*'))\n",
    "list_trade_file_train = glob.glob(os.path.join(datapath,'trade_train.parquet','*')) \n",
    "list_trade_file_test = glob.glob(os.path.join(datapath,'trade_test.parquet','*'))\n",
    "\n",
    "# Compute predictions\n",
    "predictions = prediction_function(pred='entropy',\n",
    "                                 book_path_train=list_order_book_file_train,\n",
    "                                 trade_path_train=list_trade_file_train,\n",
    "                                 targets=train,\n",
    "                                 book_path_test=list_order_book_file_test,\n",
    "                                 trade_path_test=list_trade_file_test,\n",
    "                                 all_stocks_ids=all_stocks_ids,\n",
    "                                 test_file=test)\n",
    "\n",
    "predictions.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
