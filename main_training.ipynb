{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for Kaggle - Optiver Realized Volatility Prediction\n",
    "@LaurentMombaerts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE TO SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "machine = 'local'\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lib Import / Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Parallel Computing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Maths\n",
    "from scipy.interpolate import interp1d\n",
    "# from arch import arch_model\n",
    "\n",
    "# Paths tricks\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Support code\n",
    "from support_file import *\n",
    "from information_measures import *\n",
    "\n",
    "if machine == 'local':\n",
    "    datapath = os.path.join(str(Path.home()), 'ownCloud', 'Data', 'Kaggle', 'optiver-realized-volatility-prediction')\n",
    "\n",
    "    # Load dataset\n",
    "    train = pd.read_csv(os.path.join(datapath,'train.csv')) \n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    # Load test ids\n",
    "    test = pd.read_csv(os.path.join(datapath,'test.csv'))\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "elif machine == 'kaggle':\n",
    "    \n",
    "    # Load dataset\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv') \n",
    "    all_stocks_ids_test = test['stock_id'].unique()\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "    datapath = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel_timeSplit(X,y,groups,model,splits):\n",
    "    \n",
    "    rmspe_list = []\n",
    "    \n",
    "    for random_split in range(splits):\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=.80, random_state=random_split)\n",
    "        gss.get_n_splits()\n",
    "\n",
    "        for train, test in gss.split(X, y, groups):\n",
    "            # CV definition\n",
    "            X_train, X_test = X.iloc[train,:], X.iloc[test,:]\n",
    "            y_train, y_test = y[train],y[test]\n",
    "            \n",
    "            # Model definition\n",
    "            model.fit(X_train,y_train)\n",
    "            yhat = model.predict(X_test)\n",
    "    \n",
    "            # Estimate perf\n",
    "            rmspe_list.append(rmspe(y_test, yhat))\n",
    "            \n",
    "    return rmspe_list\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    \n",
    "    df['stock_id'] = [df['row_id'][i].split('-')[0] for i in range(df.shape[0])]\n",
    "    df['time_id'] = [df['row_id'][i].split('-')[1] for i in range(df.shape[0])]\n",
    "            \n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', \n",
    "                'log_return3_realized_volatility', 'log_return4_realized_volatility', \n",
    "                'log_returnMidprice_realized_volatility',\n",
    "                'log_return1_realized_volatility_480', 'log_return2_realized_volatility_480',\n",
    "                'log_return3_realized_volatility_480', 'log_return4_realized_volatility_480',\n",
    "                'log_returnMidprice_realized_volatility_480',\n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300',\n",
    "                'log_return3_realized_volatility_300', 'log_return4_realized_volatility_300',\n",
    "                'log_returnMidprice_realized_volatility_300', \n",
    "                'log_return1_realized_volatility_120', 'log_return2_realized_volatility_120',\n",
    "                'log_return3_realized_volatility_120', 'log_return4_realized_volatility_120',\n",
    "                'log_returnMidprice_realized_volatility_120',         \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_480', \n",
    "                'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_120']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the time id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time','stock_id','time_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Prediction function (chose here which prediction strategy to use)\n",
    "def prediction_function(pred, machine, targets, all_stocks_ids, datapath, test, all_stocks_ids_test):\n",
    "        \n",
    "    if pred == 'entropy':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_train['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding_wTrades':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "        \n",
    "    if pred == 'garch':\n",
    "        \n",
    "        if machine == 'local':\n",
    "            book_path_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "            \n",
    "            # fit garch and predict\n",
    "            prediction = garch_volatility_per_stock(list_file=book_path_train, prediction_column_name='pred')\n",
    "            \n",
    "            # Merge and evaluate results\n",
    "            prediction = train.merge(prediction[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "            prediction = prediction[prediction.pred.notnull()]\n",
    "\n",
    "            # Estimate performances\n",
    "            R2 = round(r2_score(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "            RMSPE = round(rmspe(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "\n",
    "            print('--')\n",
    "            print(f'Performance of prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n",
    "\n",
    "            prediction = prediction.drop(columns=['target'])\n",
    "            prediction = prediction.rename(columns={'pred': 'target'})\n",
    "            \n",
    "            return prediction\n",
    "        \n",
    "\n",
    "    if pred == 'test_2807':\n",
    "        if machine == 'local':\n",
    "\n",
    "            # Load data\n",
    "            df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=[0], datapath=datapath)\n",
    "            df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            \n",
    "            # Add other stocks volatility at same time id and this stock overall volatility\n",
    "            df_features_test = get_time_stock(df_features_test)\n",
    "            df_features_train = get_time_stock(df_features_train)\n",
    "            \n",
    "            # Data input / output definition\n",
    "            X = df_features_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            time_id_groups = [df_features_train['row_id'][i].split('-')[1] for i in range(df_features_train.shape[0])]\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            \n",
    "            # K Splits on time_id for assessing performances\n",
    "            start = time.time()\n",
    "            print('Model Training on splits...')\n",
    "            list_rmspe = trainModel_timeSplit(X=X,y=y,groups=time_id_groups,model=model,splits=10)\n",
    "            print('Training on splits took ',  time.time() - start, ' seconds')\n",
    "            # Print results\n",
    "            print(list_rmspe)\n",
    "            print('Mean of RMSPE : ', np.mean(np.array(list_rmspe)), ' +- ', np.std(np.array(list_rmspe)))\n",
    "                  \n",
    "            return df_features_train # Returns the feature in local mode for further use\n",
    "\n",
    "        # Features computation\n",
    "        df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids_test, datapath=datapath)\n",
    "        df_features_test = test.merge(df_features_test, on = ['row_id'], how = 'left') # Should ensure order of predictions\n",
    "        df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Add other stocks volatility at same time id and this stock overall volatility\n",
    "        df_features_test = get_time_stock(df_features_test)\n",
    "        df_features_train = get_time_stock(df_features_train)\n",
    "            \n",
    "        # Training model\n",
    "        X = df_features_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_test['row_id'],yhat_pd],axis=1)    \n",
    "\n",
    "    return submission_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training on splits...\n",
      "Training on splits took  907.0027980804443\n",
      "[0.2584418411687802, 0.2731322491926642, 0.2648678655637787, 0.26230820436375396, 0.27281781932163524, 0.2684214135199438, 0.2617800019696548, 0.2758453894088322, 0.25185322240449415, 0.27279173854611494]\n",
      "Mean of RMSPE :  0.2662259745459652  +-  0.0073194275080202004\n"
     ]
    }
   ],
   "source": [
    "# New sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_submission = prediction_function(pred='test_2807',machine=machine,targets=train['target'],all_stocks_ids=all_stocks_ids, datapath=datapath, test=test, all_stocks_ids_test=all_stocks_ids_test)\n",
    "if machine == 'kaggle':\n",
    "    df_submission.to_csv('submission.csv',index=False)\n",
    "else:\n",
    "    df_submission.iloc[0:10,:].to_csv('features_train_head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
