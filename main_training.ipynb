{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for Kaggle - Optiver Realized Volatility Prediction\n",
    "@LaurentMombaerts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE TO SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "machine = 'local'\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lib Import / Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Parallel Computing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Maths\n",
    "from scipy.interpolate import interp1d\n",
    "# from arch import arch_model\n",
    "\n",
    "# Paths tricks\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Support code\n",
    "from support_file import *\n",
    "from information_measures import *\n",
    "\n",
    "if machine == 'local':\n",
    "    datapath = os.path.join(str(Path.home()), 'ownCloud', 'Data', 'Kaggle', 'optiver-realized-volatility-prediction')\n",
    "\n",
    "    # Load dataset\n",
    "    train = pd.read_csv(os.path.join(datapath,'train.csv')) \n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    # Load test ids\n",
    "    test = pd.read_csv(os.path.join(datapath,'test.csv'))\n",
    "    all_stocks_ids_test = test['stock_id'].unique()\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "elif machine == 'kaggle':\n",
    "    \n",
    "    # Load dataset\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv') \n",
    "    all_stocks_ids_test = test['stock_id'].unique()\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "    datapath = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel_timeSplit(X,y,groups,model,splits):\n",
    "    \n",
    "    rmspe_list = []\n",
    "    \n",
    "    for random_split in range(splits):\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=.80, random_state=random_split)\n",
    "        gss.get_n_splits()\n",
    "\n",
    "        for train, test in gss.split(X, y, groups):\n",
    "            # CV definition\n",
    "            X_train, X_test = X.iloc[train,:], X.iloc[test,:]\n",
    "            y_train, y_test = y[train],y[test]\n",
    "            \n",
    "            # Add other stocks volatility at same time id and this stock overall volatility\n",
    "            X_test = get_time_stock(X_test).drop(['row_id','time_id','stock_id'],axis=1)\n",
    "            X_train = get_time_stock(X_train).drop(['row_id','time_id','stock_id'],axis=1)\n",
    "    \n",
    "            # Model definition\n",
    "            model.fit(X_train,y_train)\n",
    "            yhat = model.predict(X_test)\n",
    "    \n",
    "            # Estimate perf\n",
    "            rmspe_list.append(rmspe(y_test, yhat))\n",
    "            \n",
    "    return rmspe_list\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    \n",
    "    df['stock_id'] = [df['row_id'][i].split('-')[0] for i in range(df.shape[0])]\n",
    "    df['time_id'] = [df['row_id'][i].split('-')[1] for i in range(df.shape[0])]\n",
    "            \n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', \n",
    "                'log_return3_realized_volatility', 'log_return4_realized_volatility', \n",
    "                'log_returnMidprice_realized_volatility',\n",
    "                'log_return1_realized_volatility_480', 'log_return2_realized_volatility_480',\n",
    "                'log_return3_realized_volatility_480', 'log_return4_realized_volatility_480',\n",
    "                'log_returnMidprice_realized_volatility_480',\n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300',\n",
    "                'log_return3_realized_volatility_300', 'log_return4_realized_volatility_300',\n",
    "                'log_returnMidprice_realized_volatility_300', \n",
    "                'log_return1_realized_volatility_120', 'log_return2_realized_volatility_120',\n",
    "                'log_return3_realized_volatility_120', 'log_return4_realized_volatility_120',\n",
    "                'log_returnMidprice_realized_volatility_120',         \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_480', \n",
    "                'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_120']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the time id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_CatBoost_cv(df_features_train,targets,splits):\n",
    "    \n",
    "    model = CatBoostRegressor(verbose=0)\n",
    "\n",
    "    # Data input / output definition\n",
    "    X = df_features_train.fillna(0)\n",
    "    y = targets\n",
    "    time_id_groups = [df_features_train['row_id'][i].split('-')[1] for i in range(df_features_train.shape[0])]\n",
    "   \n",
    "    rmspe_list = trainModel_timeSplit(X,y,time_id_groups,model,splits)\n",
    "    \n",
    "    return rmspe_list\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    \n",
    "    y_true = lgb_train.get_label()\n",
    "    \n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_lgbm_cv(df_features_train,targets,splits):\n",
    "    \n",
    "    # Data input / output definition\n",
    "    X = df_features_train.fillna(0)\n",
    "    y = targets\n",
    "    time_id_groups = [df_features_train['row_id'][i].split('-')[1] for i in range(df_features_train.shape[0])]\n",
    "\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'num_leaves': 100,\n",
    "      'n_jobs': -1,\n",
    "      'learning_rate': 0.1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "      'verbose': -1\n",
    "    }\n",
    "\n",
    "    rmspe_list = []\n",
    "\n",
    "    for random_split in range(splits):\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=.80, random_state=random_split)\n",
    "        gss.get_n_splits()\n",
    "\n",
    "        for train, test in gss.split(X, y, time_id_groups):\n",
    "            # CV definition\n",
    "            x_train, x_val = X.iloc[train,:].reset_index(drop=True), X.iloc[test,:].reset_index(drop=True)\n",
    "            y_train, y_val = y[train].reset_index(drop=True),y[test].reset_index(drop=True)\n",
    "\n",
    "            # Add other stocks volatility at same time id and this stock overall volatility\n",
    "            x_val = get_time_stock(x_val).drop(['row_id','time_id'],axis=1)\n",
    "            x_val['stock_id'] = x_val['stock_id'].astype(int)\n",
    "            x_train = get_time_stock(x_train).drop(['row_id','time_id'],axis=1)\n",
    "            x_train['stock_id'] = x_train['stock_id'].astype(int)\n",
    "\n",
    "            # Root mean squared percentage error weights\n",
    "            train_weights = 1 / np.square(y_train)\n",
    "            val_weights = 1 / np.square(y_val)\n",
    "            train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "            val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "\n",
    "            # Model definition\n",
    "            model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 10000, \n",
    "                          early_stopping_rounds = 50, \n",
    "                          verbose_eval = 50,\n",
    "                          feval = feval_rmspe)\n",
    "\n",
    "            yhat = model.predict(x_val) \n",
    "\n",
    "            # Estimate perf\n",
    "            rmspe_list.append(rmspe(y_val, yhat))\n",
    "            \n",
    "    return rmspe_list\n",
    "\n",
    "def train_lgbm(df_features_train,targets):\n",
    "    \n",
    "    # Data input / output definition\n",
    "    X = df_features_train.fillna(0)\n",
    "    X['stock_id'] = X['stock_id'].astype(int)\n",
    "    y = targets\n",
    "\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'num_leaves': 100,\n",
    "      'n_jobs': -1,\n",
    "      'learning_rate': 0.1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "      'verbose': -1\n",
    "    }\n",
    "\n",
    "    X['stock_id'] = X['stock_id'].astype(int)\n",
    "            \n",
    "    # Root mean squared percentage error weights\n",
    "    train_weights = 1 / np.square(y)\n",
    "    train_dataset = lgb.Dataset(X, y, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "\n",
    "    # Model definition\n",
    "    model = lgb.train(params = params, \n",
    "                  train_set = train_dataset, \n",
    "                  num_boost_round = 50, \n",
    "                  verbose_eval = 50,\n",
    "                  feval = feval_rmspe)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Prediction function (chose here which prediction strategy to use)\n",
    "def prediction_function(pred, machine, targets, all_stocks_ids, datapath, test, all_stocks_ids_test):\n",
    "        \n",
    "    if pred == 'entropy':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_train['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding_wTrades':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "        \n",
    "    if pred == 'garch':\n",
    "        \n",
    "        if machine == 'local':\n",
    "            book_path_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "            \n",
    "            # fit garch and predict\n",
    "            prediction = garch_volatility_per_stock(list_file=book_path_train, prediction_column_name='pred')\n",
    "            \n",
    "            # Merge and evaluate results\n",
    "            prediction = train.merge(prediction[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "            prediction = prediction[prediction.pred.notnull()]\n",
    "\n",
    "            # Estimate performances\n",
    "            R2 = round(r2_score(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "            RMSPE = round(rmspe(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "\n",
    "            print('--')\n",
    "            print(f'Performance of prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n",
    "\n",
    "            prediction = prediction.drop(columns=['target'])\n",
    "            prediction = prediction.rename(columns={'pred': 'target'})\n",
    "            \n",
    "            return prediction\n",
    "        \n",
    "\n",
    "    if pred == 'test_2807':\n",
    "        if machine == 'local':\n",
    "\n",
    "            # Load data\n",
    "            df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=[0], datapath=datapath).fillna(0)\n",
    "            df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath).fillna(0)\n",
    "                              \n",
    "            # Modelling\n",
    "            start = time.time()\n",
    "            print('Model Training on splits...')\n",
    "            \n",
    "            # Model list\n",
    "            #list_rmspe = train_CatBoost_cv(df_features_train=df_features_train,targets=targets,splits=10)\n",
    "            list_rmspe = train_lgbm_cv(df_features_train=df_features_train,targets=targets,splits=10)\n",
    "            \n",
    "            print('Training on splits took ',  time.time() - start, ' seconds')\n",
    "            \n",
    "            # Print results\n",
    "            print(list_rmspe)\n",
    "            print('Mean of RMSPE : ', np.mean(np.array(list_rmspe)), ' +- ', np.std(np.array(list_rmspe)))\n",
    "            \n",
    "            return df_features_train # Returns the feature in local mode for further use\n",
    "\n",
    "        # Features computation\n",
    "        df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids_test, datapath=datapath).fillna(0)\n",
    "        df_features_test = test.merge(df_features_test, on = ['row_id'], how = 'left') # Should ensure order of predictions\n",
    "        df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath).fillna(0)\n",
    "        rows_id_to_merge = df_features_test['row_id'].copy()\n",
    "        \n",
    "        # Add other stocks volatility at same time id and this stock overall volatility\n",
    "        df_features_test = get_time_stock(df_features_test).drop(['row_id','time_id'],axis=1)\n",
    "        df_features_test['stock_id'] = df_features_test['stock_id'].astype(int)\n",
    "        df_features_train = get_time_stock(df_features_train).drop(['row_id','time_id'],axis=1)\n",
    "        \n",
    "        # Optimized model\n",
    "        model = train_lgbm(df_features_train=df_features_train, targets=targets)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        yhat = model.predict(df_features_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([rows_id_to_merge,yhat_pd],axis=1)    \n",
    "\n",
    "    return submission_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training on splits...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000430677\ttraining's RMSPE: 0.199661\tvalid_1's rmse: 0.000485679\tvalid_1's RMSPE: 0.223248\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmse: 0.000436691\ttraining's RMSPE: 0.202449\tvalid_1's rmse: 0.000485188\tvalid_1's RMSPE: 0.223023\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000428833\ttraining's RMSPE: 0.19959\tvalid_1's rmse: 0.000506094\tvalid_1's RMSPE: 0.228859\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's rmse: 0.000442605\ttraining's RMSPE: 0.206\tvalid_1's rmse: 0.000504916\tvalid_1's RMSPE: 0.228326\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000430034\ttraining's RMSPE: 0.198642\tvalid_1's rmse: 0.000484979\tvalid_1's RMSPE: 0.226176\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's rmse: 0.000440526\ttraining's RMSPE: 0.203488\tvalid_1's rmse: 0.000483068\tvalid_1's RMSPE: 0.225285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000430303\ttraining's RMSPE: 0.199164\tvalid_1's rmse: 0.000491397\tvalid_1's RMSPE: 0.227362\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's rmse: 0.000439998\ttraining's RMSPE: 0.203651\tvalid_1's rmse: 0.000491331\tvalid_1's RMSPE: 0.227331\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000429405\ttraining's RMSPE: 0.198752\tvalid_1's rmse: 0.000502755\tvalid_1's RMSPE: 0.232597\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmse: 0.000435525\ttraining's RMSPE: 0.201585\tvalid_1's rmse: 0.000499884\tvalid_1's RMSPE: 0.231268\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000431182\ttraining's RMSPE: 0.199523\tvalid_1's rmse: 0.000482054\tvalid_1's RMSPE: 0.223252\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's rmse: 0.000443154\ttraining's RMSPE: 0.205062\tvalid_1's rmse: 0.000480498\tvalid_1's RMSPE: 0.222531\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000425646\ttraining's RMSPE: 0.198775\tvalid_1's rmse: 0.000511368\tvalid_1's RMSPE: 0.227911\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's rmse: 0.000444438\ttraining's RMSPE: 0.207551\tvalid_1's rmse: 0.000509878\tvalid_1's RMSPE: 0.227247\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000429015\ttraining's RMSPE: 0.199099\tvalid_1's rmse: 0.000508157\tvalid_1's RMSPE: 0.232582\n",
      "[100]\ttraining's rmse: 0.000394047\ttraining's RMSPE: 0.182871\tvalid_1's rmse: 0.00050934\tvalid_1's RMSPE: 0.233124\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's rmse: 0.000409942\ttraining's RMSPE: 0.190247\tvalid_1's rmse: 0.000507353\tvalid_1's RMSPE: 0.232214\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000426878\ttraining's RMSPE: 0.198638\tvalid_1's rmse: 0.000500105\tvalid_1's RMSPE: 0.226357\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's rmse: 0.000446838\ttraining's RMSPE: 0.207926\tvalid_1's rmse: 0.000496758\tvalid_1's RMSPE: 0.224842\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000430137\ttraining's RMSPE: 0.198782\tvalid_1's rmse: 0.000491838\tvalid_1's RMSPE: 0.228955\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's rmse: 0.000453683\ttraining's RMSPE: 0.209663\tvalid_1's rmse: 0.000489022\tvalid_1's RMSPE: 0.227644\n",
      "Training on splits took  464.30107593536377  seconds\n",
      "[0.22302268077996096, 0.22832633364424568, 0.22528518404277756, 0.227331043557769, 0.2312684509289081, 0.22253129339795225, 0.22724664965147273, 0.23221429532280877, 0.224841795472875, 0.22764409645923586]\n",
      "Mean of RMSPE :  0.22697118232580057  +-  0.003018800075463662\n"
     ]
    }
   ],
   "source": [
    "# New sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_submission = prediction_function(pred='test_2807',machine=machine,targets=train['target'],all_stocks_ids=all_stocks_ids, datapath=datapath, test=test, all_stocks_ids_test=all_stocks_ids_test)\n",
    "if machine == 'kaggle':\n",
    "    df_submission.to_csv('submission.csv',index=False)\n",
    "else:\n",
    "    df_submission.iloc[0:10,:].to_csv('features_train_head.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
