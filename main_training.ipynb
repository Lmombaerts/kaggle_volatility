{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for Kaggle - Optiver Realized Volatility Prediction\n",
    "@LaurentMombaerts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lib Import / Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Maths\n",
    "import nolds\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Paths tricks\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Support code\n",
    "from support_file import *\n",
    "\n",
    "datapath = os.path.join(str(Path.home()), 'ownCloud', 'Data', 'Kaggle', 'optiver-realized-volatility-prediction')\n",
    "\n",
    "# Load dataset\n",
    "train = pd.read_csv(os.path.join(datapath,'train.csv')) \n",
    "all_stocks_ids = train['stock_id'].unique()\n",
    "all_time_ids = train['time_id'].unique()\n",
    "\n",
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "train = train[['row_id','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Prediction function (chose here which prediction strategy to use)\n",
    "def prediction_function(pred,book_path_train,trade_path_train,targets,book_path_test,trade_path_test):\n",
    "    \n",
    "    if pred == 'naive':\n",
    "        # Naive prediction (persistence model)\n",
    "        prediction = past_realized_volatility_per_stock(list_file=book_path_train,prediction_column_name='pred')\n",
    "        \n",
    "        # Merge and evaluate results\n",
    "        prediction = train.merge(prediction[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "        print(prediction.head(5))\n",
    "\n",
    "        # Estimate performances\n",
    "        R2 = round(r2_score(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "        RMSPE = round(rmspe(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "\n",
    "        print('--')\n",
    "        print(f'Performance of prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n",
    "        \n",
    "        prediction = prediction.drop(columns=['target'])\n",
    "        prediction = prediction.rename(columns={'pred': 'target'})\n",
    "\n",
    "    if pred == 'stupid_RF':\n",
    "        # Stupid nonlinear regression between persistence and next volatility (random forest)\n",
    "        prediction = stupidForestPrediction(book_path_train=book_path_train,\n",
    "                                            prediction_column_name='pred',\n",
    "                                            train_targets_pd=targets,\n",
    "                                            book_path_test=book_path_test)\n",
    "        \n",
    "    if pred == 'entropy_based':\n",
    "        prediction = entropy_Prediction(book_path_train=book_path_train,\n",
    "                                            prediction_column_name='pred',\n",
    "                                            train_targets_pd=targets,\n",
    "                                            book_path_test=book_path_test)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a prediction code\n",
    "\n",
    "# Glob book file train (contains all paths for each file in this folder)\n",
    "list_order_book_file_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "list_order_book_file_test = glob.glob(os.path.join(datapath,'book_test.parquet','*'))\n",
    "list_trade_file_train = glob.glob(os.path.join(datapath,'trade_train.parquet','*')) \n",
    "list_trade_file_test = glob.glob(os.path.join(datapath,'trade_test.parquet','*'))\n",
    "\n",
    "# Given variables\n",
    "pred = 'entropy_based'\n",
    "book_path_train = list_order_book_file_train\n",
    "trade_path_train = list_trade_file_train\n",
    "targets = train\n",
    "book_path_test = list_order_book_file_test\n",
    "trade_path_test = list_trade_file_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficient version\n",
    "#book_all_features = pd.DataFrame()\n",
    "#encoder = np.eye(len(all_stocks_ids))\n",
    "\n",
    "#for file in book_path_train:\n",
    "\n",
    "#file = book_path_train[0]\n",
    "#start = time.time()\n",
    "\n",
    "#book_stock = pd.read_parquet(file)\n",
    "#stock_id = file.split('=')[1]\n",
    "#print('stock id computing = ' + str(stock_id) + '...')\n",
    "\n",
    "# Compute outside of loops\n",
    "#book_stock['wap'] = compute_wap(book_stock)\n",
    "#book_stock['log_return'] = book_stock.groupby(['time_id'])['wap'].apply(log_return)\n",
    "#book_stock = book_stock[~book_stock['log_return'].isnull()]\n",
    "\n",
    "#print(book_stock.head(5))\n",
    "\n",
    "# Compute the square root of the sum of log return squared to get realized volatility\n",
    "#realized_vol = book_stock.groupby(['time_id'])['log_return'].agg(realized_volatility)\n",
    "#df_realized_vol_per_stock = pd.DataFrame(realized_vol)\n",
    "#df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':'realized_volatility'})\n",
    "\n",
    "#entropy = book_stock.groupby(['time_id']).agg(entropy_from_book,last_min=10)\n",
    "#entropy_5 = book_stock.groupby(['time_id']).agg(entropy_from_book,last_min=5)\n",
    "#entropy_2 = book_stock.groupby(['time_id']).agg(entropy_from_book,last_min=2)\n",
    "\n",
    "#print(entropy_2.head(5))\n",
    "\n",
    "#encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]   \n",
    "#encoded_stock_pd = pd.DataFrame(encoded_stock)\n",
    "\n",
    "# Concatenate features, rows\n",
    "#book_features = pd.concat([book_features,encoded_stock_pd],axis=1)\n",
    "#book_all_features = pd.concat([book_all_features,book_features])\n",
    "        \n",
    "#print('Computing one stock entropy took', time.time() - start, 'seconds for stock ', stock_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock id computing = 17\n",
      "Computing one stock entropy took 31.456115007400513 seconds for stock  17\n",
      "stock id computing = 28\n",
      "Computing one stock entropy took 33.643455028533936 seconds for stock  28\n",
      "stock id computing = 10\n",
      "Computing one stock entropy took 36.56559920310974 seconds for stock  10\n",
      "stock id computing = 26\n",
      "Computing one stock entropy took 37.351609230041504 seconds for stock  26\n",
      "stock id computing = 19\n",
      "Computing one stock entropy took 38.20047688484192 seconds for stock  19\n",
      "stock id computing = 21\n",
      "Computing one stock entropy took 40.42672276496887 seconds for stock  21\n",
      "stock id computing = 75\n",
      "Computing one stock entropy took 40.034929275512695 seconds for stock  75\n",
      "stock id computing = 81\n",
      "Computing one stock entropy took 42.548349142074585 seconds for stock  81\n",
      "stock id computing = 86\n",
      "Computing one stock entropy took 49.011104583740234 seconds for stock  86\n",
      "stock id computing = 72\n",
      "Computing one stock entropy took 47.08795690536499 seconds for stock  72\n",
      "stock id computing = 44\n",
      "Computing one stock entropy took 54.32077383995056 seconds for stock  44\n",
      "stock id computing = 88\n",
      "Computing one stock entropy took 54.055014848709106 seconds for stock  88\n",
      "stock id computing = 43\n",
      "Computing one stock entropy took 58.53552007675171 seconds for stock  43\n",
      "stock id computing = 20\n",
      "Computing one stock entropy took 59.81764101982117 seconds for stock  20\n",
      "stock id computing = 27\n",
      "Computing one stock entropy took 60.14413070678711 seconds for stock  27\n",
      "stock id computing = 18\n",
      "Computing one stock entropy took 61.915229082107544 seconds for stock  18\n",
      "stock id computing = 11\n",
      "Computing one stock entropy took 65.42688703536987 seconds for stock  11\n",
      "stock id computing = 16\n",
      "Computing one stock entropy took 66.3993730545044 seconds for stock  16\n",
      "stock id computing = 29\n",
      "Computing one stock entropy took 908.1446342468262 seconds for stock  29\n",
      "stock id computing = 89\n",
      "Computing one stock entropy took 1139.3577270507812 seconds for stock  89\n",
      "stock id computing = 42\n",
      "Computing one stock entropy took 1489.6976718902588 seconds for stock  42\n",
      "stock id computing = 73\n",
      "Computing one stock entropy took 1019.0040261745453 seconds for stock  73\n",
      "stock id computing = 87\n",
      "Computing one stock entropy took 2137.952131986618 seconds for stock  87\n",
      "stock id computing = 80\n",
      "Computing one stock entropy took 2208.148267030716 seconds for stock  80\n",
      "stock id computing = 74\n",
      "Computing one stock entropy took 2141.694576025009 seconds for stock  74\n",
      "stock id computing = 103\n",
      "Computing one stock entropy took 1212.9465079307556 seconds for stock  103\n",
      "stock id computing = 104\n",
      "Computing one stock entropy took 2095.9932248592377 seconds for stock  104\n",
      "stock id computing = 105\n",
      "Computing one stock entropy took 2049.4467368125916 seconds for stock  105\n",
      "stock id computing = 102\n",
      "Computing one stock entropy took 2238.589177131653 seconds for stock  102\n",
      "stock id computing = 120\n",
      "Computing one stock entropy took 2098.4473221302032 seconds for stock  120\n",
      "stock id computing = 118\n",
      "Computing one stock entropy took 1022.1526658535004 seconds for stock  118\n",
      "stock id computing = 111\n"
     ]
    }
   ],
   "source": [
    "book_all_features = pd.DataFrame()\n",
    "encoder = np.eye(len(all_stocks_ids))\n",
    "\n",
    "for file in book_path_train:\n",
    "    start = time.time()\n",
    "\n",
    "    #file = book_path_train[6]\n",
    "    book_stock = pd.read_parquet(file)\n",
    "    stock_id = file.split('=')[1]\n",
    "    print('stock id computing = ' + str(stock_id))\n",
    "    stock_time_ids = book_stock['time_id'].unique()\n",
    "    for time_id in stock_time_ids:     \n",
    "        \n",
    "        # Access book data at this time + stock\n",
    "        book_stock_time = book_stock[book_stock['time_id'] == time_id]\n",
    "\n",
    "        # Create feature matrix\n",
    "        book_features = pd.DataFrame()\n",
    "        book_features['stock_id'] = [stock_id]\n",
    "        book_features['time_id'] = [time_id]\n",
    "        book_features['row_id'] = book_features['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "\n",
    "        # Hand-designed features\n",
    "        book_features['volatility'] = realized_volatility_from_book_pd(book_stock_time=book_stock_time)\n",
    "        #book_features['entropy'] = entropy_from_book(book_stock_time=book_stock_time,last_min=10)  \n",
    "        #book_features['entropy_last5'] = entropy_from_book(book_stock_time=book_stock_time,last_min=5)\n",
    "        book_features['entropy_last2'] = entropy_from_book(book_stock_time=book_stock_time,last_min=2)\n",
    "\n",
    "        encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]   \n",
    "        encoded_stock_pd = pd.DataFrame(encoded_stock)\n",
    "\n",
    "        # Concatenate features, rows\n",
    "        book_features = pd.concat([book_features,encoded_stock_pd],axis=1)   \n",
    "        book_all_features = pd.concat([book_all_features,book_features])\n",
    "    \n",
    "    print('Computing one stock entropy took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "# Merge targets\n",
    "#book_all_features = book_all_features.merge(train, on = ['row_id'])\n",
    "book_all_features = train.merge(book_all_features, on = ['row_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    5,    11,    16, ..., 32758, 32763, 32767], dtype=int16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_stock['time_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:16:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Persistence model perf : 0.39322701284497674\n",
      "New model perf :  0.32213065513839995\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X = book_all_features.drop(['row_id','target','stock_id','time_id'],axis=1)\n",
    "y = book_all_features['target']\n",
    "\n",
    "x_test = X # to change\n",
    "\n",
    "xgboost_default = xgb.XGBRegressor(random_state=0)\n",
    "xgboost_default.fit(X,y)\n",
    "\n",
    "yhat = xgboost_default.predict(x_test)\n",
    "print('Persistence model perf :', rmspe(y,book_all_features['volatility']))\n",
    "print('New model perf : ', rmspe(y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main evaluation code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glob book file train (contains all paths for each file in this folder)\n",
    "list_order_book_file_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "list_order_book_file_test = glob.glob(os.path.join(datapath,'book_test.parquet','*'))\n",
    "list_trade_file_train = glob.glob(os.path.join(datapath,'trade_train.parquet','*')) \n",
    "list_trade_file_test = glob.glob(os.path.join(datapath,'trade_test.parquet','*'))\n",
    "\n",
    "# Compute predictions\n",
    "prediction = prediction_function(pred='stupid_RF',\n",
    "                                 book_path_train=list_order_book_file_train,\n",
    "                                 trade_path_train=list_trade_file_train,\n",
    "                                 targets=train,\n",
    "                                 book_path_test=list_order_book_file_test,\n",
    "                                 trade_path_test=list_trade_file_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001062"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
