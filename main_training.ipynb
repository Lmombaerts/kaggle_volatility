{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for Kaggle - Optiver Realized Volatility Prediction\n",
    "@LaurentMombaerts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE TO SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "machine = 'local'\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lib Import / Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Maths\n",
    "from scipy.interpolate import interp1d\n",
    "# from arch import arch_model\n",
    "\n",
    "# Paths tricks\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Support code\n",
    "from support_file import *\n",
    "from information_measures import *\n",
    "\n",
    "if machine == 'local':\n",
    "    datapath = os.path.join(str(Path.home()), 'ownCloud', 'Data', 'Kaggle', 'optiver-realized-volatility-prediction')\n",
    "\n",
    "    # Load dataset\n",
    "    train = pd.read_csv(os.path.join(datapath,'train.csv')) \n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    # Load test ids\n",
    "    test = pd.read_csv(os.path.join(datapath,'test.csv'))\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "elif machine == 'kaggle':\n",
    "    \n",
    "    # Load dataset\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv') \n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "    datapath = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Prediction function (chose here which prediction strategy to use)\n",
    "def prediction_function(pred, machine, targets, all_stocks_ids, datapath):\n",
    "        \n",
    "    if pred == 'entropy':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_train['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding_wTrades':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "        \n",
    "    if pred == 'garch':\n",
    "        \n",
    "        if machine == 'local':\n",
    "            book_path_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "            \n",
    "            # fit garch and predict\n",
    "            prediction = garch_volatility_per_stock(list_file=book_path_train, prediction_column_name='pred')\n",
    "            \n",
    "            # Merge and evaluate results\n",
    "            prediction = train.merge(prediction[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "            prediction = prediction[prediction.pred.notnull()]\n",
    "\n",
    "            # Estimate performances\n",
    "            R2 = round(r2_score(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "            RMSPE = round(rmspe(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "\n",
    "            print('--')\n",
    "            print(f'Performance of prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n",
    "\n",
    "            prediction = prediction.drop(columns=['target'])\n",
    "            prediction = prediction.rename(columns={'pred': 'target'})\n",
    "            \n",
    "            return prediction\n",
    "        \n",
    "    if pred == 'new_version':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_july(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_july(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_train['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_july(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_july(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "\n",
    "    return submission_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing one stock took 0.7710602283477783 seconds for stock  0\n",
      "Computing one stock took 21.529297351837158 seconds for stock  0\n",
      "Computing one stock took 22.306584119796753 seconds for stock  1\n",
      "Computing one stock took 22.503216981887817 seconds for stock  2\n",
      "Computing one stock took 22.554168939590454 seconds for stock  3\n",
      "Computing one stock took 22.087051153182983 seconds for stock  4\n",
      "Computing one stock took 22.01963186264038 seconds for stock  5\n",
      "Computing one stock took 22.310144186019897 seconds for stock  6\n",
      "Computing one stock took 21.86559820175171 seconds for stock  7\n",
      "Computing one stock took 22.24520492553711 seconds for stock  8\n",
      "Computing one stock took 22.276533365249634 seconds for stock  9\n",
      "Computing one stock took 23.402235746383667 seconds for stock  10\n",
      "Computing one stock took 22.36283278465271 seconds for stock  11\n",
      "Computing one stock took 22.92237424850464 seconds for stock  13\n",
      "Computing one stock took 23.051005125045776 seconds for stock  14\n",
      "Computing one stock took 22.616986751556396 seconds for stock  15\n",
      "Computing one stock took 22.113922834396362 seconds for stock  16\n",
      "Computing one stock took 22.65027403831482 seconds for stock  17\n",
      "Computing one stock took 21.698925018310547 seconds for stock  18\n",
      "Computing one stock took 21.88465976715088 seconds for stock  19\n",
      "Computing one stock took 22.422425270080566 seconds for stock  20\n",
      "Computing one stock took 22.60633420944214 seconds for stock  21\n",
      "Computing one stock took 22.087416172027588 seconds for stock  22\n",
      "Computing one stock took 22.066044092178345 seconds for stock  23\n",
      "Computing one stock took 22.28361701965332 seconds for stock  26\n",
      "Computing one stock took 22.018163919448853 seconds for stock  27\n",
      "Computing one stock took 21.90397310256958 seconds for stock  28\n",
      "Computing one stock took 22.878226041793823 seconds for stock  29\n",
      "Computing one stock took 21.864397287368774 seconds for stock  30\n",
      "Computing one stock took 22.237375020980835 seconds for stock  31\n",
      "Computing one stock took 22.422732830047607 seconds for stock  32\n",
      "Computing one stock took 21.39560294151306 seconds for stock  33\n",
      "Computing one stock took 22.03447985649109 seconds for stock  34\n",
      "Computing one stock took 22.628532886505127 seconds for stock  35\n",
      "Computing one stock took 22.314868927001953 seconds for stock  36\n",
      "Computing one stock took 21.38421082496643 seconds for stock  37\n",
      "Computing one stock took 21.524013996124268 seconds for stock  38\n",
      "Computing one stock took 22.11520290374756 seconds for stock  39\n",
      "Computing one stock took 21.491454124450684 seconds for stock  40\n",
      "Computing one stock took 22.549401998519897 seconds for stock  41\n",
      "Computing one stock took 21.814892053604126 seconds for stock  42\n",
      "Computing one stock took 22.820435047149658 seconds for stock  43\n",
      "Computing one stock took 22.542216062545776 seconds for stock  44\n",
      "Computing one stock took 22.31090807914734 seconds for stock  46\n",
      "Computing one stock took 22.587504863739014 seconds for stock  47\n",
      "Computing one stock took 21.75862717628479 seconds for stock  48\n",
      "Computing one stock took 22.59659504890442 seconds for stock  50\n",
      "Computing one stock took 22.088233947753906 seconds for stock  51\n",
      "Computing one stock took 22.051635265350342 seconds for stock  52\n",
      "Computing one stock took 21.802241802215576 seconds for stock  53\n",
      "Computing one stock took 21.58069109916687 seconds for stock  55\n",
      "Computing one stock took 22.326208114624023 seconds for stock  56\n",
      "Computing one stock took 21.624481201171875 seconds for stock  58\n",
      "Computing one stock took 21.785671949386597 seconds for stock  59\n",
      "Computing one stock took 21.403114080429077 seconds for stock  60\n",
      "Computing one stock took 22.36090111732483 seconds for stock  61\n",
      "Computing one stock took 21.75119400024414 seconds for stock  62\n",
      "Computing one stock took 22.418477058410645 seconds for stock  63\n",
      "Computing one stock took 22.137955904006958 seconds for stock  64\n",
      "Computing one stock took 21.587801218032837 seconds for stock  66\n",
      "Computing one stock took 22.078999042510986 seconds for stock  67\n",
      "Computing one stock took 21.90556812286377 seconds for stock  68\n",
      "Computing one stock took 22.854169845581055 seconds for stock  69\n",
      "Computing one stock took 21.75262999534607 seconds for stock  70\n",
      "Computing one stock took 21.656601905822754 seconds for stock  72\n",
      "Computing one stock took 21.973533868789673 seconds for stock  73\n",
      "Computing one stock took 21.883711099624634 seconds for stock  74\n",
      "Computing one stock took 21.508965253829956 seconds for stock  75\n",
      "Computing one stock took 22.15057682991028 seconds for stock  76\n",
      "Computing one stock took 22.616358041763306 seconds for stock  77\n",
      "Computing one stock took 21.778269052505493 seconds for stock  78\n",
      "Computing one stock took 21.879239082336426 seconds for stock  80\n",
      "Computing one stock took 21.949465036392212 seconds for stock  81\n",
      "Computing one stock took 21.619797945022583 seconds for stock  82\n",
      "Computing one stock took 21.5055890083313 seconds for stock  83\n",
      "Computing one stock took 22.361319065093994 seconds for stock  84\n",
      "Computing one stock took 21.919009923934937 seconds for stock  85\n",
      "Computing one stock took 22.09778094291687 seconds for stock  86\n",
      "Computing one stock took 21.769330978393555 seconds for stock  87\n",
      "Computing one stock took 21.401090145111084 seconds for stock  88\n",
      "Computing one stock took 21.73524785041809 seconds for stock  89\n",
      "Computing one stock took 21.553462982177734 seconds for stock  90\n",
      "Computing one stock took 22.16489815711975 seconds for stock  93\n",
      "Computing one stock took 21.740433931350708 seconds for stock  94\n",
      "Computing one stock took 22.335014820098877 seconds for stock  95\n",
      "Computing one stock took 22.140247106552124 seconds for stock  96\n",
      "Computing one stock took 21.588191986083984 seconds for stock  97\n",
      "Computing one stock took 21.508596420288086 seconds for stock  98\n",
      "Computing one stock took 22.182352781295776 seconds for stock  99\n",
      "Computing one stock took 21.775688886642456 seconds for stock  100\n",
      "Computing one stock took 22.006492137908936 seconds for stock  101\n",
      "Computing one stock took 21.653331995010376 seconds for stock  102\n",
      "Computing one stock took 21.488112211227417 seconds for stock  103\n",
      "Computing one stock took 21.711852073669434 seconds for stock  104\n",
      "Computing one stock took 21.86285710334778 seconds for stock  105\n",
      "Computing one stock took 21.91513967514038 seconds for stock  107\n",
      "Computing one stock took 22.632768869400024 seconds for stock  108\n",
      "Computing one stock took 21.94118022918701 seconds for stock  109\n",
      "Computing one stock took 21.759156227111816 seconds for stock  110\n",
      "Computing one stock took 22.667373180389404 seconds for stock  111\n",
      "Computing one stock took 21.443679094314575 seconds for stock  112\n",
      "Computing one stock took 22.076592922210693 seconds for stock  113\n",
      "Computing one stock took 21.759126901626587 seconds for stock  114\n",
      "Computing one stock took 21.68898630142212 seconds for stock  115\n",
      "Computing one stock took 21.689961910247803 seconds for stock  116\n",
      "Computing one stock took 21.603724002838135 seconds for stock  118\n",
      "Computing one stock took 22.58079719543457 seconds for stock  119\n",
      "Computing one stock took 22.3020441532135 seconds for stock  120\n",
      "Computing one stock took 21.951333045959473 seconds for stock  122\n",
      "Computing one stock took 22.4795401096344 seconds for stock  123\n",
      "Computing one stock took 22.856137990951538 seconds for stock  124\n",
      "Computing one stock took 22.55841302871704 seconds for stock  125\n",
      "Computing one stock took 21.602225065231323 seconds for stock  126\n",
      "New model catboost perf :  0.26530880312960575\n"
     ]
    }
   ],
   "source": [
    "# New sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_submission = prediction_function(pred='new_test_laurent_withoutEncoding_wTrades',machine=machine,targets=train['target'],all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "df_submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing one stock took 0.8217861652374268 seconds for stock  0\n",
      "Computing one stock took 36.58018517494202 seconds for stock  0\n",
      "Computing one stock took 37.26157021522522 seconds for stock  1\n",
      "Computing one stock took 37.62245202064514 seconds for stock  2\n",
      "Computing one stock took 37.35794997215271 seconds for stock  3\n",
      "Computing one stock took 36.98911905288696 seconds for stock  4\n",
      "Computing one stock took 37.34412598609924 seconds for stock  5\n",
      "Computing one stock took 37.54107093811035 seconds for stock  6\n",
      "Computing one stock took 37.58751177787781 seconds for stock  7\n",
      "Computing one stock took 38.015620946884155 seconds for stock  8\n",
      "Computing one stock took 36.760937213897705 seconds for stock  9\n",
      "Computing one stock took 37.910584926605225 seconds for stock  10\n",
      "Computing one stock took 38.25134634971619 seconds for stock  11\n",
      "Computing one stock took 38.303149938583374 seconds for stock  13\n",
      "Computing one stock took 38.78213286399841 seconds for stock  14\n",
      "Computing one stock took 38.32510805130005 seconds for stock  15\n",
      "Computing one stock took 37.05711102485657 seconds for stock  16\n",
      "Computing one stock took 37.27394390106201 seconds for stock  17\n",
      "Computing one stock took 37.111923933029175 seconds for stock  18\n",
      "Computing one stock took 37.15374302864075 seconds for stock  19\n",
      "Computing one stock took 37.59502601623535 seconds for stock  20\n",
      "Computing one stock took 37.6738440990448 seconds for stock  21\n",
      "Computing one stock took 37.22744798660278 seconds for stock  22\n",
      "Computing one stock took 37.35340690612793 seconds for stock  23\n",
      "Computing one stock took 37.45968198776245 seconds for stock  26\n",
      "Computing one stock took 36.846306800842285 seconds for stock  27\n",
      "Computing one stock took 37.337175130844116 seconds for stock  28\n",
      "Computing one stock took 38.46440601348877 seconds for stock  29\n",
      "Computing one stock took 36.96066999435425 seconds for stock  30\n",
      "Computing one stock took 37.70276212692261 seconds for stock  31\n",
      "Computing one stock took 38.00489807128906 seconds for stock  32\n",
      "Computing one stock took 36.811208963394165 seconds for stock  33\n",
      "Computing one stock took 37.497984886169434 seconds for stock  34\n",
      "Computing one stock took 38.1838641166687 seconds for stock  35\n",
      "Computing one stock took 37.898415088653564 seconds for stock  36\n",
      "Computing one stock took 36.82686185836792 seconds for stock  37\n",
      "Computing one stock took 36.99431490898132 seconds for stock  38\n",
      "Computing one stock took 37.714454889297485 seconds for stock  39\n",
      "Computing one stock took 37.03837704658508 seconds for stock  40\n",
      "Computing one stock took 38.06297540664673 seconds for stock  41\n",
      "Computing one stock took 37.398918867111206 seconds for stock  42\n",
      "Computing one stock took 38.59085488319397 seconds for stock  43\n",
      "Computing one stock took 38.114009857177734 seconds for stock  44\n",
      "Computing one stock took 37.82099199295044 seconds for stock  46\n",
      "Computing one stock took 37.83890700340271 seconds for stock  47\n",
      "Computing one stock took 37.15910720825195 seconds for stock  48\n",
      "Computing one stock took 38.2514488697052 seconds for stock  50\n",
      "Computing one stock took 37.608500957489014 seconds for stock  51\n",
      "Computing one stock took 37.5389769077301 seconds for stock  52\n",
      "Computing one stock took 37.355839014053345 seconds for stock  53\n",
      "Computing one stock took 37.36832523345947 seconds for stock  55\n",
      "Computing one stock took 37.817660331726074 seconds for stock  56\n",
      "Computing one stock took 37.12617897987366 seconds for stock  58\n",
      "Computing one stock took 37.32362389564514 seconds for stock  59\n",
      "Computing one stock took 36.77020502090454 seconds for stock  60\n",
      "Computing one stock took 37.72829794883728 seconds for stock  61\n",
      "Computing one stock took 37.12405180931091 seconds for stock  62\n",
      "Computing one stock took 37.830251932144165 seconds for stock  63\n",
      "Computing one stock took 37.591850996017456 seconds for stock  64\n",
      "Computing one stock took 36.89300584793091 seconds for stock  66\n",
      "Computing one stock took 37.51078414916992 seconds for stock  67\n",
      "Computing one stock took 37.387824296951294 seconds for stock  68\n",
      "Computing one stock took 38.32063889503479 seconds for stock  69\n",
      "Computing one stock took 37.23344278335571 seconds for stock  70\n",
      "Computing one stock took 37.09106183052063 seconds for stock  72\n",
      "Computing one stock took 37.382239818573 seconds for stock  73\n",
      "Computing one stock took 37.218079805374146 seconds for stock  74\n",
      "Computing one stock took 36.763670921325684 seconds for stock  75\n",
      "Computing one stock took 37.58111310005188 seconds for stock  76\n",
      "Computing one stock took 38.15100812911987 seconds for stock  77\n",
      "Computing one stock took 37.09135627746582 seconds for stock  78\n",
      "Computing one stock took 37.24256110191345 seconds for stock  80\n",
      "Computing one stock took 37.35030913352966 seconds for stock  81\n",
      "Computing one stock took 36.900774002075195 seconds for stock  82\n",
      "Computing one stock took 36.853659868240356 seconds for stock  83\n",
      "Computing one stock took 37.81034278869629 seconds for stock  84\n",
      "Computing one stock took 37.38249087333679 seconds for stock  85\n",
      "Computing one stock took 37.62743592262268 seconds for stock  86\n",
      "Computing one stock took 37.16547894477844 seconds for stock  87\n",
      "Computing one stock took 36.76219391822815 seconds for stock  88\n",
      "Computing one stock took 37.18163704872131 seconds for stock  89\n",
      "Computing one stock took 36.90648317337036 seconds for stock  90\n",
      "Computing one stock took 37.603705167770386 seconds for stock  93\n",
      "Computing one stock took 37.04678225517273 seconds for stock  94\n",
      "Computing one stock took 37.68651485443115 seconds for stock  95\n",
      "Computing one stock took 37.48628306388855 seconds for stock  96\n",
      "Computing one stock took 36.86514496803284 seconds for stock  97\n",
      "Computing one stock took 36.786283016204834 seconds for stock  98\n",
      "Computing one stock took 37.66624474525452 seconds for stock  99\n",
      "Computing one stock took 37.116642236709595 seconds for stock  100\n",
      "Computing one stock took 37.41396999359131 seconds for stock  101\n",
      "Computing one stock took 37.06765604019165 seconds for stock  102\n",
      "Computing one stock took 36.7785370349884 seconds for stock  103\n",
      "Computing one stock took 37.151602029800415 seconds for stock  104\n",
      "Computing one stock took 37.231427907943726 seconds for stock  105\n",
      "Computing one stock took 37.12294292449951 seconds for stock  107\n",
      "Computing one stock took 37.98850679397583 seconds for stock  108\n",
      "Computing one stock took 37.316741943359375 seconds for stock  109\n",
      "Computing one stock took 37.1461398601532 seconds for stock  110\n",
      "Computing one stock took 38.10636377334595 seconds for stock  111\n",
      "Computing one stock took 36.85541391372681 seconds for stock  112\n",
      "Computing one stock took 37.44079804420471 seconds for stock  113\n",
      "Computing one stock took 37.13363289833069 seconds for stock  114\n",
      "Computing one stock took 37.04354786872864 seconds for stock  115\n",
      "Computing one stock took 36.90312600135803 seconds for stock  116\n",
      "Computing one stock took 36.92825198173523 seconds for stock  118\n",
      "Computing one stock took 38.056715965270996 seconds for stock  119\n",
      "Computing one stock took 37.73706912994385 seconds for stock  120\n",
      "Computing one stock took 37.38745903968811 seconds for stock  122\n",
      "Computing one stock took 37.984780073165894 seconds for stock  123\n",
      "Computing one stock took 38.29442310333252 seconds for stock  124\n",
      "Computing one stock took 37.66199207305908 seconds for stock  125\n",
      "Computing one stock took 36.97720503807068 seconds for stock  126\n",
      "New model catboost perf :  0.2649384144902905\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_submission = prediction_function(pred='new_test_laurent_withoutEncoding_wTrades',machine=machine,targets=train['target'],all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "df_submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
