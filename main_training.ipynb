{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for Kaggle - Optiver Realized Volatility Prediction\n",
    "@LaurentMombaerts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE TO SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "machine = 'local'\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lib Import / Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Maths\n",
    "from scipy.interpolate import interp1d\n",
    "# from arch import arch_model\n",
    "\n",
    "# Paths tricks\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Support code\n",
    "from support_file import *\n",
    "from information_measures import *\n",
    "\n",
    "if machine == 'local':\n",
    "    datapath = os.path.join(str(Path.home()), 'ownCloud', 'Data', 'Kaggle', 'optiver-realized-volatility-prediction')\n",
    "\n",
    "    # Load dataset\n",
    "    train = pd.read_csv(os.path.join(datapath,'train.csv')) \n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    # Load test ids\n",
    "    test = pd.read_csv(os.path.join(datapath,'test.csv'))\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "elif machine == 'kaggle':\n",
    "    \n",
    "    # Load dataset\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv') \n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "    datapath = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel_timeSplit(X,y,groups,model,splits):\n",
    "    \n",
    "    rmspe_list = []\n",
    "    \n",
    "    for random_split in range(splits):\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=.80, random_state=random_split)\n",
    "        gss.get_n_splits()\n",
    "\n",
    "        for train, test in gss.split(X, y, groups):\n",
    "            # CV definition\n",
    "            X_train, X_test = X.iloc[train,:], X.iloc[test,:]\n",
    "            y_train, y_test = y[train],y[test]\n",
    "            \n",
    "            # Model definition\n",
    "            model.fit(X_train,y_train)\n",
    "            yhat = model.predict(X_test)\n",
    "    \n",
    "            # Estimate perf\n",
    "            rmspe_list.append(rmspe(y_test, yhat))\n",
    "            \n",
    "    return rmspe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Prediction function (chose here which prediction strategy to use)\n",
    "def prediction_function(pred, machine, targets, all_stocks_ids, datapath):\n",
    "        \n",
    "    if pred == 'entropy':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_train['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding_wTrades':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "        \n",
    "    if pred == 'garch':\n",
    "        \n",
    "        if machine == 'local':\n",
    "            book_path_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "            \n",
    "            # fit garch and predict\n",
    "            prediction = garch_volatility_per_stock(list_file=book_path_train, prediction_column_name='pred')\n",
    "            \n",
    "            # Merge and evaluate results\n",
    "            prediction = train.merge(prediction[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "            prediction = prediction[prediction.pred.notnull()]\n",
    "\n",
    "            # Estimate performances\n",
    "            R2 = round(r2_score(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "            RMSPE = round(rmspe(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "\n",
    "            print('--')\n",
    "            print(f'Performance of prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n",
    "\n",
    "            prediction = prediction.drop(columns=['target'])\n",
    "            prediction = prediction.rename(columns={'pred': 'target'})\n",
    "            \n",
    "            return prediction\n",
    "        \n",
    "\n",
    "    if pred == 'test_2807':\n",
    "        if machine == 'local':\n",
    "            \n",
    "            # Load data\n",
    "            df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            \n",
    "            X = df_features_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Groups\n",
    "            temp = df_features_train.copy()\n",
    "            temp['row_id'] = [temp['row_id'][i].split('-')[1] for i in range(temp.shape[0])]\n",
    "            temp = temp.rename(columns={'row_id':'time_id'})\n",
    "            groups = temp['time_id']\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            \n",
    "            # K Splits on time_id for assessing performances\n",
    "            list_rmspe = trainModel_timeSplit(X=X,y=y,groups=groups,model=model,splits=10)\n",
    "            print(list_rmspe)\n",
    "            print('Mean of RMSPE : ', np.mean(np.array(list_rmspe)), ' +- ', np.std(np.array(list_rmspe)))\n",
    "                  \n",
    "            return df_features_train # Returns the feature in local mode for further use\n",
    "\n",
    "        # Features computation\n",
    "        df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_test['row_id'],yhat_pd],axis=1)    \n",
    "\n",
    "    return submission_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing one stock took 0.1633288860321045 seconds for stock  0\n",
      "Computing one stock took 55.83275508880615 seconds for stock  0\n",
      "Computing one stock took 57.324979066848755 seconds for stock  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9a15e476065b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_2807'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_stocks_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_stocks_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-635a7985bd11>\u001b[0m in \u001b[0;36mprediction_function\u001b[0;34m(pred, machine, targets, all_stocks_ids, datapath)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mdf_features_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeFeatures_2807\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_stocks_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_stocks_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mdf_features_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeFeatures_2807\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_stocks_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_stocks_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_features_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'row_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/kaggle_volatility/support_file.py\u001b[0m in \u001b[0;36mcomputeFeatures_2807\u001b[0;34m(machine, dataset, all_stocks_ids, datapath)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0;31m# Book stats processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1428\u001b[0;31m         \u001b[0mbook_features_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_stock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1429\u001b[0m         \u001b[0mbook_features_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_features_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/kaggle_volatility/support_file.py\u001b[0m in \u001b[0;36mbook_preprocessor\u001b[0;34m(book_stock, stock_id)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0mdf_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds_in_bucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m     \u001b[0mdf_feature_480\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds_in_bucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m     \u001b[0mdf_feature_300\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds_in_bucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m     \u001b[0mdf_feature_120\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds_in_bucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/kaggle_volatility/support_file.py\u001b[0m in \u001b[0;36mget_stats_window\u001b[0;34m(seconds_in_bucket, add_suffix)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0;31m# Group by the window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m         \u001b[0mdf_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_stock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook_stock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seconds_in_bucket'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mseconds_in_bucket\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_feature_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m         \u001b[0;31m# Rename columns joining suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_agg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_agg_1dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_agg\u001b[0;34m(arg, func)\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_how\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_how\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_agg_1dim\u001b[0;34m(name, how, subset)\u001b[0m\n\u001b[1;32m    365\u001b[0m                         \u001b[0;34m\"nested dictionary is ambiguous in aggregation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     )\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcolg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_agg_2dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;31m# but not the class list / tuple itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_multiple_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrelabeling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_aggregate_multiple_funcs\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mcyfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcyfunc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcyfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnkeys\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mstd\u001b[0;34m(self, ddof)\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mcython_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m             \u001b[0mpost_processing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1450\u001b[0;31m             \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1451\u001b[0m         )\n\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml_dl_p37/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_get_cythonized_result\u001b[0;34m(self, how, cython_dtype, aggregate, numeric_only, needs_counts, needs_values, needs_2d, min_count, needs_mask, needs_ngroups, result_is_index, pre_processing, post_processing, **kwargs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m                 \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2507\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Call func to modify indexer values in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mneeds_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_submission = prediction_function(pred='test_2807',machine=machine,targets=train['target'],all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "df_submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
