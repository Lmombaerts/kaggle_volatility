{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Main code for Kaggle - Optiver Realized Volatility Prediction\n",
    "@LaurentMombaerts "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%reset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MACHINE TO SET UP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "###########################\n",
    "machine = 'local'\n",
    "###########################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Lib Import / Data loading**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Parallel Computing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Maths\n",
    "from scipy.interpolate import interp1d\n",
    "# from arch import arch_model\n",
    "\n",
    "# Paths tricks\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Support code\n",
    "from support_file import *\n",
    "from information_measures import *\n",
    "\n",
    "if machine == 'local':\n",
    "    datapath = os.path.join(str(Path.home()), 'ownCloud', 'Data', 'Kaggle', 'optiver-realized-volatility-prediction')\n",
    "\n",
    "    # Load dataset\n",
    "    train = pd.read_csv(os.path.join(datapath,'train.csv')) \n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    # Load test ids\n",
    "    test = pd.read_csv(os.path.join(datapath,'test.csv'))\n",
    "    all_stocks_ids_test = test['stock_id'].unique()\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "elif machine == 'kaggle':\n",
    "    \n",
    "    # Load dataset\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    all_stocks_ids = train['stock_id'].unique()\n",
    "    all_time_ids = train['time_id'].unique()\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    train = train[['row_id','target']]\n",
    "\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv') \n",
    "    all_stocks_ids_test = test['stock_id'].unique()\n",
    "    test = test.drop(['stock_id','time_id'],axis=1)\n",
    "    \n",
    "    datapath = 0\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Functions**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "def trainModel_timeSplit(X,y,groups,model,splits):\n",
    "    \n",
    "    rmspe_list = []\n",
    "    \n",
    "    for random_split in range(splits):\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=.80, random_state=random_split)\n",
    "        gss.get_n_splits()\n",
    "\n",
    "        for train, test in gss.split(X, y, groups):\n",
    "            # CV definition\n",
    "            X_train, X_test = X.iloc[train,:], X.iloc[test,:]\n",
    "            y_train, y_test = y[train],y[test]\n",
    "            \n",
    "            # Add other stocks volatility at same time id and this stock overall volatility\n",
    "            X_test = get_time_stock(X_test).drop(['time_id','stock_id'],axis=1)\n",
    "            X_train = get_time_stock(X_train).drop(['time_id','stock_id'],axis=1)\n",
    "    \n",
    "            # Model definition\n",
    "            model.fit(X_train,y_train)\n",
    "            yhat = model.predict(X_test)\n",
    "    \n",
    "            # Estimate perf\n",
    "            rmspe_list.append(rmspe(y_test, yhat))\n",
    "            \n",
    "    return rmspe_list\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    \n",
    "    # df['stock_id'] = [df['row_id'][i].split('-')[0] for i in range(df.shape[0])]\n",
    "    # df['time_id'] = [df['row_id'][i].split('-')[1] for i in range(df.shape[0])]\n",
    "            \n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_returnMidprice_realized_volatility']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the time id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_CatBoost_cv(df_features_train,targets,splits):\n",
    "    \n",
    "    model = CatBoostRegressor(verbose=0)\n",
    "\n",
    "    # Data input / output definition\n",
    "    X = df_features_train.fillna(0)\n",
    "    y = targets\n",
    "    time_id_groups = [df_features_train['row_id'][i].split('-')[1] for i in range(df_features_train.shape[0])]\n",
    "   \n",
    "    rmspe_list = trainModel_timeSplit(X,y,time_id_groups,model,splits)\n",
    "    \n",
    "    return rmspe_list\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    \n",
    "    y_true = lgb_train.get_label()\n",
    "    \n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_lgbm_cv(df_features_train,targets,splits):\n",
    "    \n",
    "    # Data input / output definition\n",
    "    X = df_features_train.fillna(0)\n",
    "    y = targets\n",
    "    time_id_groups = [df_features_train['time_id'][i] for i in range(df_features_train.shape[0])]\n",
    "\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'num_leaves': 100,\n",
    "      'n_jobs': -1,\n",
    "      'learning_rate': 0.1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "      'verbose': -1\n",
    "    }\n",
    "\n",
    "    rmspe_list = []\n",
    "    rmspe_list_29 = []\n",
    "\n",
    "    for random_split in range(splits):\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=.80, random_state=random_split)\n",
    "        gss.get_n_splits()\n",
    "\n",
    "        for train, test in gss.split(X, y, time_id_groups):\n",
    "            # CV definition\n",
    "            x_train, x_val = X.iloc[train,:].reset_index(drop=True), X.iloc[test,:].reset_index(drop=True)\n",
    "            y_train, y_val = y[train].reset_index(drop=True),y[test].reset_index(drop=True)\n",
    "            y_val_29 = y_val[x_val['stock_id']==29]\n",
    "\n",
    "            # Add other stocks volatility at same time id and this stock overall volatility\n",
    "            x_val = get_time_stock(x_val).drop(['time_id'],axis=1)\n",
    "            x_val['stock_id'] = x_val['stock_id'].astype(int)\n",
    "            x_train = get_time_stock(x_train).drop(['time_id'],axis=1)\n",
    "            x_train['stock_id'] = x_train['stock_id'].astype(int)\n",
    "\n",
    "            # Root mean squared percentage error weights\n",
    "            train_weights = 1 / np.square(y_train)\n",
    "            val_weights = 1 / np.square(y_val)\n",
    "            train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "            val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "\n",
    "            # Model definition\n",
    "            model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 10000, \n",
    "                          early_stopping_rounds = 50, \n",
    "                          verbose_eval = 50,\n",
    "                          feval = feval_rmspe)\n",
    "\n",
    "            yhat = model.predict(x_val)\n",
    "            yhat_29 = model.predict(x_val[x_val['stock_id']==29])\n",
    "\n",
    "            # Estimate perf\n",
    "            rmspe_list.append(rmspe(y_val, yhat))\n",
    "            rmspe_list_29.append(rmspe(y_val_29, yhat_29))\n",
    "            \n",
    "    return pd.DataFrame(list(zip(rmspe_list, rmspe_list_29)), columns=['rmspe', 'rmspe29'])\n",
    "\n",
    "def train_lgbm(df_features_train,targets):\n",
    "    \n",
    "    # Data input / output definition\n",
    "    X = df_features_train.fillna(0)\n",
    "    X['stock_id'] = X['stock_id'].astype(int)\n",
    "    y = targets\n",
    "\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'num_leaves': 100,\n",
    "      'n_jobs': -1,\n",
    "      'learning_rate': 0.1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "      'verbose': -1\n",
    "    }\n",
    "\n",
    "    X['stock_id'] = X['stock_id'].astype(int)\n",
    "            \n",
    "    # Root mean squared percentage error weights\n",
    "    train_weights = 1 / np.square(y)\n",
    "    train_dataset = lgb.Dataset(X, y, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "\n",
    "    # Model definition\n",
    "    model = lgb.train(params = params, \n",
    "                  train_set = train_dataset, \n",
    "                  num_boost_round = 50, \n",
    "                  verbose_eval = 50,\n",
    "                  feval = feval_rmspe)\n",
    "            \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Competition metric\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Prediction function (chose here which prediction strategy to use)\n",
    "def prediction_function(pred, machine, targets, all_stocks_ids, datapath, test, all_stocks_ids_test):\n",
    "        \n",
    "    if pred == 'entropy':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_train['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_wEntropy(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_wEntropy(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_noCode(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "    if pred == 'new_test_laurent_withoutEncoding_wTrades':\n",
    "        if machine == 'local':\n",
    "            # Load data\n",
    "            df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "            X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "            y = targets\n",
    "            \n",
    "            # Model\n",
    "            model = CatBoostRegressor(verbose=0)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # Predicting targets from same\n",
    "            yhat = model.predict(X)\n",
    "            \n",
    "            print('New model catboost perf : ', rmspe(y, yhat))\n",
    "            \n",
    "            # Submission file\n",
    "            yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "            return pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "\n",
    "        # Features computation\n",
    "        df_features_encoded_test = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        df_features_encoded_train = computeFeatures_newTest_Laurent_wTrades(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath)\n",
    "        \n",
    "        # Training model\n",
    "        X = df_features_encoded_train.drop(['row_id'],axis=1)\n",
    "        y = targets\n",
    "        \n",
    "        # Optimized model\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        X_test = df_features_encoded_test.drop(['row_id'],axis=1)\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([df_features_encoded_test['row_id'],yhat_pd],axis=1)\n",
    "        \n",
    "        \n",
    "    if pred == 'garch':\n",
    "        \n",
    "        if machine == 'local':\n",
    "            book_path_train = glob.glob(os.path.join(datapath,'book_train.parquet','*')) \n",
    "            \n",
    "            # fit garch and predict\n",
    "            prediction = garch_volatility_per_stock(list_file=book_path_train, prediction_column_name='pred')\n",
    "            \n",
    "            # Merge and evaluate results\n",
    "            prediction = train.merge(prediction[['row_id','pred']], on = ['row_id'], how = 'left')\n",
    "            prediction = prediction[prediction.pred.notnull()]\n",
    "\n",
    "            # Estimate performances\n",
    "            R2 = round(r2_score(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "            RMSPE = round(rmspe(y_true = prediction['target'], y_pred = prediction['pred']),3)\n",
    "\n",
    "            print('--')\n",
    "            print(f'Performance of prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n",
    "\n",
    "            prediction = prediction.drop(columns=['target'])\n",
    "            prediction = prediction.rename(columns={'pred': 'target'})\n",
    "            \n",
    "            return prediction\n",
    "        \n",
    "\n",
    "    if pred == 'test_2807':\n",
    "        if machine == 'local':\n",
    "\n",
    "            # Load data\n",
    "            df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=[0], datapath=datapath).fillna(0)\n",
    "            df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath).fillna(0)\n",
    "\n",
    "            # saving features to the .csv file\n",
    "            print('['+time.strftime('%X')+']', 'Saving features to .csv files ...') # print also time\n",
    "            df_features_train.to_csv('df_features_train.csv')\n",
    "            df_features_test.to_csv('df_features_test.csv')\n",
    "                              \n",
    "            # Modelling\n",
    "            start = time.time()\n",
    "            print('['+time.strftime('%X')+']', 'Model Training on splits...')\n",
    "            \n",
    "            # Model list\n",
    "            #list_rmspe = train_CatBoost_cv(df_features_train=df_features_train,targets=targets,splits=10)\n",
    "            list_rmspe = train_lgbm_cv(df_features_train=df_features_train,targets=targets,splits=10)\n",
    "            \n",
    "            print('['+time.strftime('%X')+']', 'Training on splits took ',  time.time() - start, 'seconds')\n",
    "            \n",
    "            # Print results\n",
    "            print(list_rmspe)\n",
    "            print('Mean of RMSPE : ', np.mean(np.array(list_rmspe)), ' +- ', np.std(np.array(list_rmspe)))\n",
    "            \n",
    "            return df_features_train # Returns the feature in local mode for further use\n",
    "\n",
    "        # Features computation\n",
    "        df_features_test = computeFeatures_2807(machine=machine, dataset='test', all_stocks_ids=all_stocks_ids_test, datapath=datapath).fillna(0)\n",
    "        df_features_test = test.merge(df_features_test, on = ['row_id'], how = 'left') # Should ensure order of predictions\n",
    "        df_features_train = computeFeatures_2807(machine=machine, dataset='train', all_stocks_ids=all_stocks_ids, datapath=datapath).fillna(0)\n",
    "        rows_id_to_merge = df_features_test['row_id'].copy()\n",
    "        \n",
    "        # Add other stocks volatility at same time id and this stock overall volatility\n",
    "        df_features_test = get_time_stock(df_features_test).drop(['row_id','time_id'],axis=1)\n",
    "        df_features_test['stock_id'] = df_features_test['stock_id'].astype(int)\n",
    "        df_features_train = get_time_stock(df_features_train).drop(['row_id','time_id'],axis=1)\n",
    "        \n",
    "        # Optimized model\n",
    "        model = train_lgbm(df_features_train=df_features_train, targets=targets)\n",
    "        \n",
    "        # Predicting targets from test\n",
    "        yhat = model.predict(df_features_test)\n",
    "        \n",
    "        # Submission file\n",
    "        yhat_pd = pd.DataFrame(yhat,columns=['target'])\n",
    "        submission_file = pd.concat([rows_id_to_merge,yhat_pd],axis=1)    \n",
    "\n",
    "    if pred == 'naive':\n",
    "        if machine == 'local':\n",
    "            df_naive_pred = past_realized_volatility_per_stock(list_file=glob.glob(os.path.join(datapath, 'book_train.parquet','*')), prediction_column_name='prediction')\n",
    "            df_joined = train.merge(df_naive_pred, on='row_id', how='left')\n",
    "            print('RMSPE = {rmspe(y_true = df_joined[\"target\"], y_pred = df_joined[\"prediction\"])}')\n",
    "            \n",
    "            return df_joined\n",
    "    submission_file = []\n",
    "\n",
    "\n",
    "    return submission_file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Submission**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# New sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_submission = prediction_function(pred='test_2807',machine=machine,targets=train['target'],all_stocks_ids=all_stocks_ids, datapath=datapath, test=test, all_stocks_ids_test=all_stocks_ids_test)\n",
    "# if machine == 'kaggle':\n",
    "#     df_submission.to_csv('submission.csv',index=False)\n",
    "# else:\n",
    "#     df_submission.iloc[0:10,:].to_csv('features_train_head.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try other stuff"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# load precomputed features\n",
    "features_temp = pd.read_csv('df_features_train.csv')\n",
    "del features_temp['Unnamed: 0']\n",
    "features_temp['stock_id'] = features_temp['row_id'].apply(lambda x: int(x.split('-')[0]))\n",
    "features_temp['time_id']  = features_temp['row_id'].apply(lambda x: int(x.split('-')[1]))\n",
    "del features_temp['row_id']\n",
    "features_temp.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(428932, 37)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# train using lgbm and check performance for all vs stock_id==29\n",
    "our_method = train_lgbm_cv(features_temp, train['target'], splits=10)\n",
    "np.mean(our_method) # 16% lower error"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000455949\ttraining's RMSPE: 0.211293\tvalid_1's rmse: 0.000513567\tvalid_1's RMSPE: 0.236448\n",
      "[100]\ttraining's rmse: 0.000421443\ttraining's RMSPE: 0.195302\tvalid_1's rmse: 0.000513366\tvalid_1's RMSPE: 0.236355\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's rmse: 0.00043607\ttraining's RMSPE: 0.20208\tvalid_1's rmse: 0.000510599\tvalid_1's RMSPE: 0.235081\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000455307\ttraining's RMSPE: 0.210638\tvalid_1's rmse: 0.000492729\tvalid_1's RMSPE: 0.228402\n",
      "[100]\ttraining's rmse: 0.000420848\ttraining's RMSPE: 0.194696\tvalid_1's rmse: 0.000498428\tvalid_1's RMSPE: 0.231044\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's rmse: 0.000448081\ttraining's RMSPE: 0.207296\tvalid_1's rmse: 0.000492358\tvalid_1's RMSPE: 0.22823\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000456235\ttraining's RMSPE: 0.210857\tvalid_1's rmse: 0.000498049\tvalid_1's RMSPE: 0.231787\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's rmse: 0.000459741\ttraining's RMSPE: 0.212477\tvalid_1's rmse: 0.000497845\tvalid_1's RMSPE: 0.231692\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000458152\ttraining's RMSPE: 0.210508\tvalid_1's rmse: 0.000488198\tvalid_1's RMSPE: 0.232352\n",
      "[100]\ttraining's rmse: 0.000422583\ttraining's RMSPE: 0.194165\tvalid_1's rmse: 0.000490523\tvalid_1's RMSPE: 0.233459\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's rmse: 0.00045162\ttraining's RMSPE: 0.207507\tvalid_1's rmse: 0.00048733\tvalid_1's RMSPE: 0.231939\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000453385\ttraining's RMSPE: 0.210599\tvalid_1's rmse: 0.000513246\tvalid_1's RMSPE: 0.234035\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's rmse: 0.000458723\ttraining's RMSPE: 0.213078\tvalid_1's rmse: 0.000512256\tvalid_1's RMSPE: 0.233584\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000449014\ttraining's RMSPE: 0.209609\tvalid_1's rmse: 0.000524824\tvalid_1's RMSPE: 0.234295\n",
      "[100]\ttraining's rmse: 0.000416931\ttraining's RMSPE: 0.194632\tvalid_1's rmse: 0.000531939\tvalid_1's RMSPE: 0.237471\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's rmse: 0.000449014\ttraining's RMSPE: 0.209609\tvalid_1's rmse: 0.000524824\tvalid_1's RMSPE: 0.234295\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000453873\ttraining's RMSPE: 0.210067\tvalid_1's rmse: 0.000495339\tvalid_1's RMSPE: 0.229212\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's rmse: 0.000456097\ttraining's RMSPE: 0.211096\tvalid_1's rmse: 0.000495244\tvalid_1's RMSPE: 0.229168\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.00045198\ttraining's RMSPE: 0.209421\tvalid_1's rmse: 0.000505214\tvalid_1's RMSPE: 0.232751\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's rmse: 0.00045951\ttraining's RMSPE: 0.212909\tvalid_1's rmse: 0.000503609\tvalid_1's RMSPE: 0.232011\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000451597\ttraining's RMSPE: 0.20968\tvalid_1's rmse: 0.000505072\tvalid_1's RMSPE: 0.230708\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's rmse: 0.000460868\ttraining's RMSPE: 0.213985\tvalid_1's rmse: 0.000503648\tvalid_1's RMSPE: 0.230058\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000455734\ttraining's RMSPE: 0.210194\tvalid_1's rmse: 0.000503493\tvalid_1's RMSPE: 0.236202\n",
      "[100]\ttraining's rmse: 0.00042066\ttraining's RMSPE: 0.194017\tvalid_1's rmse: 0.00050535\tvalid_1's RMSPE: 0.237073\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's rmse: 0.00045155\ttraining's RMSPE: 0.208264\tvalid_1's rmse: 0.00050315\tvalid_1's RMSPE: 0.236041\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run persistence prediction and join with targets to compute RMSPE\n",
    "test_df = past_realized_volatility_per_stock(list_file=glob.glob(os.path.join(datapath, 'book_train.parquet','*')), prediction_column_name='prediction')\n",
    "df_joined = train.merge(test_df, on='row_id', how='left')\n",
    "df_joined['stock_id'] = df_joined['row_id'].apply(lambda x: int(x.split('-')[0]))\n",
    "df_joined"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# Compute and print RMSPE (31% lower error)\n",
    "persistence_rmspe = rmspe(df_joined['target'], df_joined['prediction'])\n",
    "persistence_rmspe_29 = rmspe(df_joined[df_joined['stock_id']==29]['target'], df_joined[df_joined['stock_id']==29]['prediction'])\n",
    "print([persistence_rmspe, persistence_rmspe_29])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.34135449018801606, 0.2327475775825411]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add clusters to the `features_temp`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# read files\n",
    "stock_clusters = pd.read_csv('stock_clusters.csv')\n",
    "time_clusters = pd.read_csv('time_clusters.csv')\n",
    "stock_clusters_vol = pd.read_csv('stock_clusters_volatility.csv')\n",
    "\n",
    "# merge the tables into features_temp\n",
    "features_temp = pd.merge(features_temp, stock_clusters, on='stock_id', how='left')\n",
    "features_temp = pd.merge(features_temp, time_clusters, on='time_id', how='left')\n",
    "features_temp = pd.merge(features_temp, stock_clusters_vol, on='stock_id', how='left')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# normalize the data (only for appropriate columns)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data = features_temp.loc[:, features_temp.columns[0:35]]\n",
    "scaler = preprocessing.StandardScaler().fit(data)\n",
    "data_scaled = scaler.transform(data)\n",
    "features_scaled = pd.concat([pd.DataFrame(data_scaled, columns=data.columns), features_temp.loc[:,features_temp.columns[35:]]], axis=1)\n",
    "features_scaled"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         wap_sum  wap_mean   wap_std  mid_price_sum  mid_price_mean  \\\n",
       "0      -0.639031  1.104316 -0.396660      -0.639001        1.111899   \n",
       "1      -1.397873  0.069885 -0.806592      -1.397924        0.060385   \n",
       "2      -1.487201 -0.136938 -0.233761      -1.487174       -0.130757   \n",
       "3      -1.988207 -0.347661 -0.336160      -1.988175       -0.336548   \n",
       "4      -1.575404 -0.114151 -0.810551      -1.575427       -0.119067   \n",
       "...          ...       ...       ...            ...             ...   \n",
       "428927 -0.589373 -0.124951 -0.593530      -0.589716       -0.170105   \n",
       "428928 -1.224837  0.733630  0.146578      -1.225015        0.704073   \n",
       "428929 -0.983920  0.320113 -0.613047      -0.984409        0.244283   \n",
       "428930  0.072101  0.535701 -0.621808       0.072293        0.556692   \n",
       "428931 -1.272639  0.079579 -0.690180      -1.272608        0.086069   \n",
       "\n",
       "        mid_price_std  log_return1_sum  log_return1_realized_volatility  \\\n",
       "0           -0.469451         0.619973                         0.074238   \n",
       "1           -0.803573         0.100188                        -0.844596   \n",
       "2           -0.319051        -0.554959                        -0.519973   \n",
       "3           -0.444075        -0.757878                        -0.462721   \n",
       "4           -0.848177         0.002744                        -0.652162   \n",
       "...               ...              ...                              ...   \n",
       "428927      -0.588372        -0.138693                        -0.151275   \n",
       "428928       0.191491         1.196938                        -0.036057   \n",
       "428929      -0.598003         0.413562                        -0.311090   \n",
       "428930      -0.640814         0.072112                        -0.159511   \n",
       "428931      -0.703082        -0.192303                        -0.597245   \n",
       "\n",
       "        log_return1_mean  log_return1_std  ...  trade_avg_trade_size  \\\n",
       "0               0.705678         0.150592  ...             -0.469105   \n",
       "1               0.169450        -0.693876  ...             -0.542342   \n",
       "2              -1.022713        -0.268272  ...             -0.435558   \n",
       "3              -2.193842         0.034539  ...             -0.418375   \n",
       "4               0.001207        -0.412300  ...             -0.571358   \n",
       "...                  ...              ...  ...                   ...   \n",
       "428927         -0.155527        -0.088900  ...             -0.515109   \n",
       "428928          1.848355         0.226598  ...             -0.621700   \n",
       "428929          0.554660        -0.160531  ...             -0.361180   \n",
       "428930          0.061596        -0.217657  ...             -0.338337   \n",
       "428931         -0.308646        -0.417282  ...             -0.231363   \n",
       "\n",
       "        stock_id  time_id  km_x  hc_x  ha_x  km_y  hc_y  ha_y  km_v  \n",
       "0              0        5     2     1     1     3     1     1     3  \n",
       "1              0       11     2     1     1     1     1     1     3  \n",
       "2              0       16     2     1     1     1     1     1     3  \n",
       "3              0       31     2     1     1     1     1     1     3  \n",
       "4              0       62     2     1     1     1     1     1     3  \n",
       "...          ...      ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "428927       126    32751     2     1     1     1     1     1     2  \n",
       "428928       126    32753     2     1     1     1     1     1     2  \n",
       "428929       126    32758     2     1     1     1     1     1     2  \n",
       "428930       126    32763     2     1     1     1     1     1     2  \n",
       "428931       126    32767     2     1     1     1     1     1     2  \n",
       "\n",
       "[428932 rows x 44 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_sum</th>\n",
       "      <th>wap_mean</th>\n",
       "      <th>wap_std</th>\n",
       "      <th>mid_price_sum</th>\n",
       "      <th>mid_price_mean</th>\n",
       "      <th>mid_price_std</th>\n",
       "      <th>log_return1_sum</th>\n",
       "      <th>log_return1_realized_volatility</th>\n",
       "      <th>log_return1_mean</th>\n",
       "      <th>log_return1_std</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_avg_trade_size</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>km_x</th>\n",
       "      <th>hc_x</th>\n",
       "      <th>ha_x</th>\n",
       "      <th>km_y</th>\n",
       "      <th>hc_y</th>\n",
       "      <th>ha_y</th>\n",
       "      <th>km_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.639031</td>\n",
       "      <td>1.104316</td>\n",
       "      <td>-0.396660</td>\n",
       "      <td>-0.639001</td>\n",
       "      <td>1.111899</td>\n",
       "      <td>-0.469451</td>\n",
       "      <td>0.619973</td>\n",
       "      <td>0.074238</td>\n",
       "      <td>0.705678</td>\n",
       "      <td>0.150592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.469105</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.397873</td>\n",
       "      <td>0.069885</td>\n",
       "      <td>-0.806592</td>\n",
       "      <td>-1.397924</td>\n",
       "      <td>0.060385</td>\n",
       "      <td>-0.803573</td>\n",
       "      <td>0.100188</td>\n",
       "      <td>-0.844596</td>\n",
       "      <td>0.169450</td>\n",
       "      <td>-0.693876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542342</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.487201</td>\n",
       "      <td>-0.136938</td>\n",
       "      <td>-0.233761</td>\n",
       "      <td>-1.487174</td>\n",
       "      <td>-0.130757</td>\n",
       "      <td>-0.319051</td>\n",
       "      <td>-0.554959</td>\n",
       "      <td>-0.519973</td>\n",
       "      <td>-1.022713</td>\n",
       "      <td>-0.268272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435558</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.988207</td>\n",
       "      <td>-0.347661</td>\n",
       "      <td>-0.336160</td>\n",
       "      <td>-1.988175</td>\n",
       "      <td>-0.336548</td>\n",
       "      <td>-0.444075</td>\n",
       "      <td>-0.757878</td>\n",
       "      <td>-0.462721</td>\n",
       "      <td>-2.193842</td>\n",
       "      <td>0.034539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.418375</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.575404</td>\n",
       "      <td>-0.114151</td>\n",
       "      <td>-0.810551</td>\n",
       "      <td>-1.575427</td>\n",
       "      <td>-0.119067</td>\n",
       "      <td>-0.848177</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>-0.652162</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>-0.412300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571358</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>-0.589373</td>\n",
       "      <td>-0.124951</td>\n",
       "      <td>-0.593530</td>\n",
       "      <td>-0.589716</td>\n",
       "      <td>-0.170105</td>\n",
       "      <td>-0.588372</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>-0.151275</td>\n",
       "      <td>-0.155527</td>\n",
       "      <td>-0.088900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.515109</td>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>-1.224837</td>\n",
       "      <td>0.733630</td>\n",
       "      <td>0.146578</td>\n",
       "      <td>-1.225015</td>\n",
       "      <td>0.704073</td>\n",
       "      <td>0.191491</td>\n",
       "      <td>1.196938</td>\n",
       "      <td>-0.036057</td>\n",
       "      <td>1.848355</td>\n",
       "      <td>0.226598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.621700</td>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>-0.983920</td>\n",
       "      <td>0.320113</td>\n",
       "      <td>-0.613047</td>\n",
       "      <td>-0.984409</td>\n",
       "      <td>0.244283</td>\n",
       "      <td>-0.598003</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>-0.311090</td>\n",
       "      <td>0.554660</td>\n",
       "      <td>-0.160531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361180</td>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.072101</td>\n",
       "      <td>0.535701</td>\n",
       "      <td>-0.621808</td>\n",
       "      <td>0.072293</td>\n",
       "      <td>0.556692</td>\n",
       "      <td>-0.640814</td>\n",
       "      <td>0.072112</td>\n",
       "      <td>-0.159511</td>\n",
       "      <td>0.061596</td>\n",
       "      <td>-0.217657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338337</td>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>-1.272639</td>\n",
       "      <td>0.079579</td>\n",
       "      <td>-0.690180</td>\n",
       "      <td>-1.272608</td>\n",
       "      <td>0.086069</td>\n",
       "      <td>-0.703082</td>\n",
       "      <td>-0.192303</td>\n",
       "      <td>-0.597245</td>\n",
       "      <td>-0.308646</td>\n",
       "      <td>-0.417282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231363</td>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows  44 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train the model with these features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_list_rmspe = train_lgbm_cv(df_features_train=features_scaled,targets=train['target'],splits=10)\n",
    "my_list_rmspe1 = train_lgbm_cv(df_features_train=features_temp,targets=train['target'],splits=10)\n",
    "my_list_rmspe2 = train_lgbm_cv(df_features_train=features_temp.loc[:, features_temp.columns[0:37]],targets=train['target'],splits=10)\n",
    "my_list_rmspe3 = train_lgbm_cv(df_features_train=features_temp.loc[:, features_temp.columns[np.r_[0:37, 43]]],targets=train['target'],splits=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "print('Mean of RMSPE (initial = 35) : ', np.mean(np.array(my_list_rmspe2)), u'\\u00B1', np.std(np.array(my_list_rmspe2)))\n",
    "print('Mean of RMSPE (initial + volatility clusters = 36) : ', np.mean(np.array(my_list_rmspe3)), u'\\u00B1', np.std(np.array(my_list_rmspe3)))\n",
    "print('Mean of RMSPE (initial + clusters = 41) : ', np.mean(np.array(my_list_rmspe1)), u'\\u00B1', np.std(np.array(my_list_rmspe1)))\n",
    "print('Mean of RMSPE (normalized + clusters = 41) : ', np.mean(np.array(my_list_rmspe)), u'\\u00B1', np.std(np.array(my_list_rmspe)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean of RMSPE (initial = 35) :  0.23220997646978137  0.002434308917999223\n",
      "Mean of RMSPE (initial + volatility clusters = 36) :  0.23273841066636133  0.0024193633074462684\n",
      "Mean of RMSPE (initial + clusters = 41) :  0.23180462747454245  0.002328161519229472\n",
      "Mean of RMSPE (normalized + clusters = 41) :  0.2318494809665371  0.0024199119214926576\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "e3af083e9e891625b9c68534a23ee03716e10120c7e3db5624663f331b3f95ba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}