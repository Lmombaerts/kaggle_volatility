{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing support_file_extension.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile support_file_extension.py\n",
    "\n",
    "def computeFeatures_newTest_Laurent(machine, dataset, all_stocks_ids, datapath):\n",
    "    \n",
    "    list_rv, list_rv2, list_rv3 = [], [], []\n",
    "    list_ent, list_fin, list_fin2 = [], [], []\n",
    "    list_others, list_others2, list_others3 = [], [], []\n",
    "\n",
    "    for stock_id in range(127):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if machine == 'local':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id(stock_id,datapath,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        elif machine == 'kaggle':\n",
    "            try:\n",
    "                book_stock = load_book_data_by_id_kaggle(stock_id,dataset)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Useful\n",
    "        all_time_ids_byStock = book_stock['time_id'].unique() \n",
    "\n",
    "        # Calculate wap for the entire book\n",
    "        book_stock['wap'] = calc_wap(book_stock)\n",
    "        book_stock['wap2'] = calc_wap2(book_stock)\n",
    "        book_stock['wap3'] = calc_wap3(book_stock)\n",
    "        book_stock['wap4'] = calc_wap2(book_stock)\n",
    "        book_stock['mid_price'] = calc_wap3(book_stock)\n",
    "\n",
    "        # Calculate past realized volatility per time_id\n",
    "        df_sub = book_stock.groupby('time_id')['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub2 = book_stock.groupby('time_id')['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub3 = book_stock.groupby('time_id')['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub4 = book_stock.groupby('time_id')['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        df_sub5 = book_stock.groupby('time_id')['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "        \n",
    "        df_sub['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub['time_id']]\n",
    "        df_sub = df_sub.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        df_sub = pd.concat([df_sub,df_sub2['wap2'],df_sub3['wap3'], df_sub4['wap4'], df_sub5['mid_price']],axis=1)\n",
    "        df_sub = df_sub.rename(columns={'wap': 'rv', 'wap2': 'rv2', 'wap3': 'rv3', 'wap4':'rv4','mid_price':'rv5'})\n",
    "        \n",
    "        list_rv.append(df_sub)\n",
    "        \n",
    "        # Query segments\n",
    "        bucketQuery480 = book_stock.query(f'seconds_in_bucket >= 480')\n",
    "        isEmpty480 = bucketQuery480.empty\n",
    "        \n",
    "        bucketQuery300 = book_stock.query(f'seconds_in_bucket >= 300')\n",
    "        isEmpty300 = bucketQuery300.empty\n",
    "        \n",
    "        times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "        times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "        times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "        \n",
    "        # Calculate past realized volatility per time_id and query subset\n",
    "        if isEmpty300 == False:\n",
    "            df_sub_300 = bucketQuery300.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_300 = bucketQuery300.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_300 = bucketQuery300.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_300 = bucketQuery300.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_300 = bucketQuery300.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_300 = pd.concat([times_pd,df_sub_300['wap'],df_sub2_300['wap2'],df_sub3_300['wap3'],df_sub4_300['wap4'],df_sub5_300['mid_range']],axis=1)\n",
    "            df_sub_300 = df_sub_300.rename(columns={'wap': 'rv_300', 'wap2_300': 'rv2', 'wap3_300': 'rv3', 'wap4':'rv4_300','mid_price':'rv5_300'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_300'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_300'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_300'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_300'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_300'])\n",
    "            df_sub_300 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "            \n",
    "        list_rv2.append(df_sub_300)\n",
    "        \n",
    "        # Calculate realized volatility last 2 min\n",
    "        if isEmpty480 == False:\n",
    "            df_sub_480 = bucketQuery480.groupby(['time_id'])['wap'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub2_480 = bucketQuery480.groupby(['time_id'])['wap2'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub3_480 = bucketQuery480.groupby(['time_id'])['wap3'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub4_480 = bucketQuery480.groupby(['time_id'])['wap4'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            df_sub5_480 = bucketQuery480.groupby(['time_id'])['mid_price'].agg(calc_rv_from_wap_numba, engine='numba').to_frame().reset_index()\n",
    "            \n",
    "\n",
    "            df_sub_480 = pd.concat([times_pd,df_sub_480['wap'],df_sub2_480['wap2'],df_sub3_480['wap3'],df_sub4_480['wap4'],df_sub5_480['mid_range']],axis=1)\n",
    "            df_sub_480 = df_sub_480.rename(columns={'wap': 'rv_480', 'wap2_480': 'rv2', 'wap3_480': 'rv3', 'wap4':'rv4_480','mid_price':'rv5_480'})\n",
    "            \n",
    "        else: # 0 volatility\n",
    "            \n",
    "            zero_rv = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv_480'])\n",
    "            zero_rv2 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv2_480'])\n",
    "            zero_rv3 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv3_480'])\n",
    "            zero_rv4 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv4_480'])\n",
    "            zero_rv5 = pd.DataFrame(np.zeros((1,times_pd.shape[0])),columns=['rv5_480'])\n",
    "            df_sub_480 = pd.concat([times_pd,zero_rv,zero_rv2,zero_rv3,zero_rv4,zero_rv5],axis=1) \n",
    "\n",
    "        \n",
    "        list_rv3.append(df_sub_480)\n",
    "\n",
    "        # Calculate other financial metrics from book \n",
    "        df_sub_book_feats = book_stock.groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={0:'embedding'})\n",
    "        df_sub_book_feats[['wap_imbalance','price_spread','bid_spread','ask_spread','total_vol','vol_imbalance']] = pd.DataFrame(df_sub_book_feats.embedding.tolist(), index=df_sub_book_feats.index)\n",
    "        df_sub_book_feats['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats['time_id']] \n",
    "        df_sub_book_feats = df_sub_book_feats.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "\n",
    "        list_fin.append(df_sub_book_feats)\n",
    "            \n",
    "        if isEmpty300 == False:\n",
    "            df_sub_book_feats_300 = book_stock.query(f'seconds_in_bucket >= 300').groupby(['time_id']).apply(financial_metrics).to_frame().reset_index()\n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={0:'embedding'})\n",
    "            df_sub_book_feats_300[['wap_imbalance5','price_spread5','bid_spread5','ask_spread5','total_vol5','vol_imbalance5']] = pd.DataFrame(df_sub_book_feats_300.embedding.tolist(), index=df_sub_book_feats_300.index)\n",
    "            df_sub_book_feats_300['time_id'] = [f'{stock_id}-{time_id}' for time_id in df_sub_book_feats_300['time_id']] \n",
    "            df_sub_book_feats_300 = df_sub_book_feats_300.rename(columns={'time_id':'row_id'}).drop(['embedding'],axis=1)\n",
    "        else:\n",
    "            times_pd = pd.DataFrame(all_time_ids_byStock,columns=['time_id'])\n",
    "            times_pd['time_id'] = [f'{stock_id}-{time_id}' for time_id in times_pd['time_id']]\n",
    "            times_pd = times_pd.rename(columns={'time_id':'row_id'})\n",
    "            temp = pd.DataFrame([0],columns=['wap_imbalance5']) \n",
    "            temp2 = pd.DataFrame([0],columns=['price_spread5'])\n",
    "            temp3 = pd.DataFrame([0],columns=['bid_spread5'])\n",
    "            temp4 = pd.DataFrame([0],columns=['ask_spread5'])\n",
    "            temp5 = pd.DataFrame([0],columns=['total_vol5'])\n",
    "            temp6 = pd.DataFrame([0],columns=['vol_imbalance5'])\n",
    "            df_sub_book_feats_300 = pd.concat([times_pd,temp,temp2,temp3,temp4,temp5,temp6],axis=1) \n",
    "            \n",
    "        list_fin2.append(df_sub_book_feats_300)\n",
    "        \n",
    "        print('Computing one stock took', time.time() - start, 'seconds for stock ', stock_id)\n",
    "\n",
    "    # Create features dataframe\n",
    "    df_submission = pd.concat(list_rv)\n",
    "    df_submission2 = pd.concat(list_rv2)\n",
    "    df_submission3 = pd.concat(list_rv3)\n",
    "    df_fin_concat = pd.concat(list_fin)\n",
    "    df_fin2_concat = pd.concat(list_fin2)\n",
    "\n",
    "    df_book_features = df_submission.merge(df_submission2, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_submission3, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    df_book_features = df_book_features.merge(df_fin2_concat, on = ['row_id'], how='left').fillna(0)\n",
    "    \n",
    "    # Add encoded stock\n",
    "    encoder = np.eye(len(all_stocks_ids))\n",
    "    encoded = list()\n",
    "\n",
    "    for i in range(df_book_features.shape[0]):\n",
    "        stock_id = int(df_book_features['row_id'][i].split('-')[0])\n",
    "        encoded_stock = encoder[np.where(all_stocks_ids == int(stock_id))[0],:]\n",
    "        encoded.append(encoded_stock)\n",
    "\n",
    "    encoded_pd = pd.DataFrame(np.array(encoded).reshape(df_book_features.shape[0],np.array(all_stocks_ids).shape[0]))\n",
    "    df_book_features_encoded = pd.concat([df_book_features, encoded_pd],axis=1)\n",
    "    \n",
    "    return df_book_features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
